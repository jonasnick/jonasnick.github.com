<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[nickler's]]></title>
  <link href="https://jonasnick.github.io/atom.xml" rel="self"/>
  <link href="https://jonasnick.github.io/"/>
  <updated>2020-01-26T14:44:54+00:00</updated>
  <id>https://jonasnick.github.io/</id>
  <author>
    <name><![CDATA[Jonas Nick]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[X-only Pubkeys and Insecure MuSig Shortcuts]]></title>
    <link href="https://jonasnick.github.io/blog/2019/11/19/x-only-pubkeys-and-insecure-musig-shortcuts/"/>
    <updated>2019-11-19T13:35:00+00:00</updated>
    <id>https://jonasnick.github.io/blog/2019/11/19/x-only-pubkeys-and-insecure-musig-shortcuts</id>
    <content type="html"><![CDATA[<p>There are two posts I recently contributed to <a href="https://medium.com/blockstream">Blockstream&rsquo;s engineering blog</a> expanding on the talk I gave at <a href="https://www.thelightningconference.com/">The Lightning Conference 2019</a>. Cross-posting them here because they fit the theme of this blog:</p>

<ul>
<li><p><a href="https://medium.com/blockstream/reducing-bitcoin-transaction-sizes-with-x-only-pubkeys-f86476af05d7">Reducing Bitcoin Transaction Sizes with x-only Pubkeys</a></p>

<blockquote><p>This article is about the recent introduction of so-called x-only pubkeys to the Bitcoin Improvement Proposal <a href="https://github.com/bitcoin/bips/blob/master/bip-0340.mediawiki">BIP-schnorr</a> [&hellip;] significantly reducing the weight of every transaction output without any loss in security. By removing the Y-coordinate byte from compressed public keys currently used in Bitcoin, public keys end up with a 32-byte representation. We are going to look at how it works, why thatâ€™s useful, and sketch a security proof.</p></blockquote></li>
<li><p><a href="https://medium.com/blockstream/insecure-shortcuts-in-musig-2ad0d38a97da">Insecure Shortcuts in MuSig</a></p>

<blockquote><p>Using BIP-Schnorr-based multisignatures, no matter how many signers are involved, the result is a single public key and a single signature indistinguishable from a regular, single-signer BIP-Schnorr signature. This article is about optimizing implementations of multisignature protocols and why seemingly harmless changes can totally break the security.</p></blockquote></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Secure protocols on BIP-taproot]]></title>
    <link href="https://jonasnick.github.io/blog/2019/06/25/secure-protocols-on-bip-taproot/"/>
    <updated>2019-06-25T22:42:00+00:00</updated>
    <id>https://jonasnick.github.io/blog/2019/06/25/secure-protocols-on-bip-taproot</id>
    <content type="html"><![CDATA[<p><a href="https://youtu.be/DKOG0BQMmmg?t=21866"><img src="https://jonasnick.github.io/images/2019-breaking.png" width="560"></a></p>

<p>At <a href="https://breaking-bitcoin.com/">Breaking Bitcoin</a> 2019 in Amsterdam I gave a talk about how to build secure protocols on BIP-taproot or more specifically how to avoid the dangers we learned about so far.
There was not enough time to cover everything.
The talk also gives an introduction to how to use our <a href="https://github.com/ElementsProject/secp256k1-zkp/blob/secp256k1-zkp/include/secp256k1_musig.h">MuSig implementation in libsecp256k1-zkp</a>.
The video recording is on <a href="https://youtu.be/DKOG0BQMmmg?t=21866">youtube</a> (<a href="https://jonasnick.github.io/slides/2019-breaking.pdf">slides</a>).
Thanks to <a href="https://twitter.com/kanzure">kanzure</a> there&rsquo;s also a <a href="https://diyhpl.us/wiki/transcripts/breaking-bitcoin/2019/secure-protocols-bip-taproot/">transcript</a> of the talk.</p>

<p><strong>Erratum</strong>: MuSig nonces can not be pre-shared. Only nonce commitments. See <a href="https://github.com/ElementsProject/secp256k1-zkp/pull/73">https://github.com/ElementsProject/secp256k1-zkp/pull/73</a> for details.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[nix-bitcoin]]></title>
    <link href="https://jonasnick.github.io/blog/2019/06/25/nix-bitcoin/"/>
    <updated>2019-06-25T22:39:00+00:00</updated>
    <id>https://jonasnick.github.io/blog/2019/06/25/nix-bitcoin</id>
    <content type="html"><![CDATA[<p><a href="https://youtu.be/BcjKejXlLbM?t=15873"><img src="https://jonasnick.github.io/images/lightninghackday-nix-bitcoin.png" width="560"></a></p>

<p>nix-bitcoin (<a href="https://github.com/fort-nix/nix-bitcoin">github.com/fort-nix/nix-bitcoin</a>) is a project I contribute to in my spare time that provides nix packages and nixos modules for easily installing Bitcoin nodes and higher layer protocols.
The initial idea was to build myself a lightning node in a reproducible way.
I talked more about the motivation and how to use it at the <a href="https://lightninghackday.fulmo.org/">LightningHackdayMUC</a> (<a href="https://youtu.be/BcjKejXlLbM?t=15873">video</a>, <a href="https://jonasnick.github.io/slides/2019-nix-bitcoin-2.pdf">slides</a>).</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Schnorr and Taproot in Lightning]]></title>
    <link href="https://jonasnick.github.io/blog/2018/09/04/schnorr-and-taproot-in-lightning/"/>
    <updated>2018-09-04T13:24:00+00:00</updated>
    <id>https://jonasnick.github.io/blog/2018/09/04/schnorr-and-taproot-in-lightning</id>
    <content type="html"><![CDATA[<p><a href="https://www.youtube.com/watch?v=QrX1SpD6l9g&feature=youtu.be&t=7524"><img src="https://jonasnick.github.io/images/lightninghackday-youtube.png" width="560"></a></p>

<p>Last weekend a bunch of hackers assembled for the 3rd Lightning Netword Hackday in Berlin.
The event was packed with interesting sessions, neat hacks and exciting discussions which were concluded with the traditional dinner &amp; drinks at ROOM77.
I gave a talk about &ldquo;Schnorr and Taproot in Lightning&rdquo; (<a href="https://jonasnick.github.io/slides/2018-hackday.pdf">slides</a>, <a href="https://www.youtube.com/watch?v=QrX1SpD6l9g&amp;feature=youtu.be&amp;t=7524">video</a>) focusing on privacy and security implications.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Blind Signatures in Scriptless Scripts]]></title>
    <link href="https://jonasnick.github.io/blog/2018/07/31/blind-signatures-in-scriptless-scripts/"/>
    <updated>2018-07-31T14:53:00+00:00</updated>
    <id>https://jonasnick.github.io/blog/2018/07/31/blind-signatures-in-scriptless-scripts</id>
    <content type="html"><![CDATA[<p><a href="https://youtu.be/XORDEX-RrAI?t=25484"><img src="https://jonasnick.github.io/images/bob-youtube.png" width="560"></a></p>

<p>At the recent Building on Bitcoin conference in Lisbon I gave a talk about a few new ideas in the scriptless scripts framework.
The first part was mainly about <a href="https://github.com/apoelstra/scriptless-scripts/pull/1">blind coinswaps</a>, which is a way to swap bitcoins with a tumbler without revealing which coin are swapped.
The second part about how to exchange ecash tokens peer-to-peer using scriptless scripts and Brands credentials.
You can find the talk <a href="https://youtu.be/XORDEX-RrAI?t=25484">on youtube</a> and the slides <a href="https://jonasnick.github.io/slides/2018-bob.pdf">here</a>.
Thanks to <a href="https://twitter.com/kanzure">kanzure</a> there&rsquo;s also a <a href="https://diyhpl.us/wiki/transcripts/building-on-bitcoin/2018/blind-signatures-and-scriptless-scripts/">transcript</a> of the talk.</p>

<p>EDIT: I&rsquo;ve added a note about the security of Blind Schnorr signatures against forgery to the <a href="https://jonasnick.github.io/slides/2018-bob.pdf">slides</a>.
In short, a naive implementation of the scheme is vulnerable to <a href="http://www.enseignement.polytechnique.fr/informatique/profs/Francois.Morain/Master1/Crypto/projects/Wagner02.pdf">Wagner&rsquo;s attack</a>.
An attacker can forge a signature using 65536 parallel signing sessions and <code>O(2^32)</code> work.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Exploiting Low Order Generators in One-Time Ring Signatures]]></title>
    <link href="https://jonasnick.github.io/blog/2017/05/23/exploiting-low-order-generators-in-one-time-ring-signatures/"/>
    <updated>2017-05-23T16:41:00+00:00</updated>
    <id>https://jonasnick.github.io/blog/2017/05/23/exploiting-low-order-generators-in-one-time-ring-signatures</id>
    <content type="html"><![CDATA[<p>Last week the Monero team disclosed <a href="https://getmonero.org/2017/05/17/disclosure-of-a-major-bug-in-cryptonote-based-currencies.html">a major bug in CryptoNote based cryptocurrencies</a> (<a href="https://www.reddit.com/r/Monero/comments/6buu5j/disclosure_of_a_major_bug_in_cryptonotebased/">reddit thread</a>) which could be used to create &ldquo;create an infinite amount of coins&rdquo;.
Monero itself was quietly fixed in February (<a href="https://github.com/monero-project/monero/releases/tag/v0.10.2">release</a>, <a href="https://github.com/monero-project/monero/pull/1744">pull request</a>) and the since then every user syncing the blockchain from scratch validates that it was never exploited in Monero.
However, it was used in CryptoNote based shitcoin ByteCoin to create about 700 million <a href="https://github.com/amjuarez/bytecoin/issues/104">coins out of thin air</a>.</p>

<p>There are already a few good explanations of the bug, for example at the <a href="https://monero.stackexchange.com/questions/4241/how-does-the-recent-patched-key-image-exploit-work-in-practice">Monero StackExchange</a> and the <a href="https://moderncrypto.org/mail-archive/curves/2017/000898.html">modern crypto mailing list</a>.
This article gives additional background about the signature scheme and the properties of the curve that allowed this bug to slip in.</p>

<p>Apart from the value at stake, this bug is interesting because it shows the risks of breaking a specialized cryptosystems such as <a href="https://ed25519.cr.yp.to/index.html">Ed25519</a> apart and apply the parts in other contexts.
Ed25519 is designed for plain cryptographic signatures and the curve it is based on is used in CryptoNote to implement <em>one-time ring signatures</em>.
In contrast to a regular signature scheme, one-time ring signatures using the curve require that part of the signature does not generate a small subgroup.
Ensuring this is necessary when <a href="https://cr.yp.to/ecdh.html#validate">using curves with a cofactor</a>. CryptoNote did not do that.</p>

<!-- more -->


<h2>Hash-based ring signatures</h2>

<p><a href="https://en.wikipedia.org/wiki/Ring_signature">A ring signature</a> proves that the signer is among a <em>set</em> of public keys (aka &ldquo;the ring&rdquo;), without revealing which public key belongs to the signer.
The construction used in CryptoNote/Monero and also for example in <a href="https://github.com/ElementsProject/secp256k1-zkp">Confidential Transactions</a> is based on hash rings.</p>

<p>For simplicity assume that for now the ring only consists of 1 key, which essentially reduces the scheme to a Schnorr signature.
As usual, <code>G</code> is a generator of cyclic group in which the discrete logarithm is hard and we&rsquo;re using additive notation for group operations.
Then the signature scheme consists of the of the following three algorithms (keygen, sign, verify):</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>keygen:
</span><span class='line'>draw scalar x uniformly at random
</span><span class='line'>P &lt;- x*G
</span><span class='line'>publish public key P
</span><span class='line'>
</span><span class='line'>sign message m (ring size 1):
</span><span class='line'>draw scalar k uniformly at random
</span><span class='line'>e &lt;- hash(m, k*G)
</span><span class='line'>s &lt;- k - e*x
</span><span class='line'>publish signature (e, s)
</span><span class='line'>
</span><span class='line'>verify signature (e, s) over message m for public key P (ring size 1):
</span><span class='line'>e == hash(m, s*G + e*P)</span></code></pre></td></tr></table></div></figure>


<p>Let&rsquo;s get a basic informal understanding for why such Schnorr based schemes work by taking the perspective of Eve, who does not know the discrete logarithm <code>x</code> of <code>P</code>.
Obviously, Eve would invalidate the signature when attempting to just change the message <code>m</code>.
Further, when trying to fake a signature without knowing <code>x</code> Eve can not to just set <code>s</code> as in the regular signing algorithm.
But to make a signature that passes verification for some public key <code>P</code> Eve must find <code>s</code>, s.t. <code>k*G = s*G + e*P</code>.
We can rearrange that in the following way:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>k*G - s*G = e*P
</span><span class='line'>(k - s)*G = e*P
</span><span class='line'>((k - s)/e)*G = P</span></code></pre></td></tr></table></div></figure>


<p>That means that if she would find such an <code>s</code> she could compute the discrete logarithm of <code>P</code>.
This is a contradiction.</p>

<p>Note that during verification the output of the hash function <code>e</code> is also part of the input to the hash function.
How about during signing Eve chooses <code>s</code> at random and then simply hashes <code>s*G + e*P</code>?
The problem is that the properties of a cryptographic hash function prevent Eve from knowing <code>e</code> before before evaluating the hash function.
So <code>e</code> can not be fed into the hash function and as a consequence <code>s</code> must be chosen to account for <code>e</code> only after hashing.</p>

<p>Rings of size one naturally don&rsquo;t make a lot of sense but are sufficient for this post&rsquo;s purpose.
The curious can for example have a look at the explanation in the <a href="https://github.com/Blockstream/borromean_paper/raw/master/borromean_draft_0.01_9ade1e49.pdf">Borromean signature paper</a>(section 2.2).</p>

<h2>One-time ring signatures</h2>

<p>One-time ring signatures are used in CryptoNote to allow combining the privacy properties of ring signatures with a mechanism to detect double spending.
This is done by introducing the concept of a &ldquo;key image&rdquo;.
The key image is a group element that is deterministically derived from a key but in itself doesn&rsquo;t reveal anything about the key.
Define <code>hashp</code> to be a hash function that hashes to an element in the group.
Then the key image <code>I</code> for the key pair <code>(x, P=x*G)</code> is <code>I = x*hashp(P)</code> so <code>P</code> and <code>I</code> have the same discrete logarithm.
A one-time ring signature includes the key image belonging to the signer.</p>

<p>The CryptoNote protocol allows using ring signatures when spending coins by enforcing that each key image can occur only once in the blockchain.
Let&rsquo;s for example assume there are two unspent coins &ndash; in our case just represented by public keys <code>P1</code> and <code>P2</code>.
Alice knows the private key to <code>P1</code>, so she can spend the coin by providing a one-time ring signature with <code>P1</code>, <code>P2</code> and the key image <code>I</code> corresponding to <code>P1</code>.
An observer can not tell whether <code>P1</code> or <code>P2</code> was spend.
But if Alice would attempt to spend <code>P1</code> again (even with a different ring) she would require the same key image which is rejected by the network.
However, <code>P2</code> can still be spent because the signature uses a different key image.</p>

<p>Now the concrete one-time ring signature scheme &ndash; again shown only for rings of size 1:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>keygen:
</span><span class='line'>draw scalar x uniformly at random
</span><span class='line'>P &lt;- x*G
</span><span class='line'>publish public key P
</span><span class='line'>
</span><span class='line'>sign message m (ring size 1):
</span><span class='line'>I &lt;- x*hashp(P)
</span><span class='line'>draw scalar k uniformly at random
</span><span class='line'>e &lt;- hash(m, k*G, k*hashp(P))
</span><span class='line'>s &lt;- k - e*x
</span><span class='line'>publish signature (I, e, s)
</span><span class='line'>
</span><span class='line'>verify signature (I, e, s) on message m for public key P (ring size 1):
</span><span class='line'>e == hash(m, s*G + e*P, s*hashp(P) + e*I)</span></code></pre></td></tr></table></div></figure>


<p>Intuitively, it&rsquo;s very hard to create a signature where <code>I != x*hashp(P)</code> because the same <code>s</code> that is used to prove knowledge of the discrete logarithm <code>x</code> of <code>P</code> is also used for <code>I</code>.</p>

<p>The one-time ring signature scheme for rings larger than 1 is described in the <a href="https://cryptonote.org/whitepaper.pdf">CryptoNote whitepaper</a>(section 4.4) although in <a href="https://bitcointalk.org/index.php?topic=972541.0">a less space efficient way</a>.
The construction is also related to the proof of discrete log equality used in <a href="https://blog.chain.com/preparing-for-a-quantum-future-45535b316314">perfectly binding Confidential Transactions</a>.
Note that actually by now a more general scheme that is based on one-time ring signatures called <a href="http://www.ledgerjournal.org/ojs/index.php/ledger/article/download/34/61">Ring Confidential Transactions</a> has replaced regular one-time ring signatures in Monero.</p>

<h2>Low order keys in Ed25519</h2>

<p>As mentioned in the beginning, CryptoNote uses Ed25519&rsquo;s curve (also referred to simply as &ldquo;Ed25519&rdquo;) to represent its group elements.
One of Ed25519&rsquo;s properties is that the number of points on the curve (curve order) is larger than the number of points in the group generated by the base point <code>G</code> (group order).
The group order is the prime <code>l = 2^252 + 27742317777372353535851937790883648493</code> and the curve order is <code>8*l</code>
The ratio of the curve order and the group order is known as the <em>cofactor</em> which is 8 in the case of Ed25519.
This is, for example, different to the curve secp256k1 used in Bitcoin which has cofactor 1.</p>

<p>The cofactor indicates that there are groups of low order on the curve.
For example, let <code>P</code> be the point represented by <code>26e8958fc2b227b045c3f489f2ef98f0d5dfac05d3c63339b13802886d53fc05</code> then <code>P</code> generates the (unique) group of order <code>8</code> which implies <code>8*P = 0</code>.
There are only few points on the curve with low order.
One way to find them is to generate a random point on the curve with order <code>a*l</code> and then multiply by <code>l</code> to get a point of order <code>1 &lt;= a &lt;= 8</code>.
On the other hand, <code>hashp</code> ensures that the resulting point is in the prime order group by multiplying it by <code>8</code> before outputting it.</p>

<p>In the case of the regular Ed25519 signature algorithm it doesn&rsquo;t matter if a public key is of low order.
But Ed25519 implementations make sure that a private key is a multiple of 8 (<a href="https://moderncrypto.org/mail-archive/curves/2017/000858.html">&ldquo;clamping&rdquo;</a>), so when multiplying it with a low order point then the result is always 0 (instead of leaking bits from the private key).
Therefore, after running the Diffie-Hellman protocol on such a curve the key shared between the two parties can be <code>0</code> if one party doesn&rsquo;t behave <a href="https://cr.yp.to/ecdh.html#validate">&ldquo;contributory&rdquo;</a>.</p>

<h2>Attack &amp; Fix</h2>

<p>Assume the attacker owns a coin and therefore can create a regular one-time ring signature to spend it with key image <code>I = x*hashp(P)</code>.
The attacker can spend the coin again with key <code>I' = I + L</code> where <code>L</code> is a low order point with order <code>o</code>.
Remember the verification equation includes <code>s*hashp(P) + e*I</code>.
If <code>o</code> divides <code>e</code>, (<code>e = e'*o</code>) then <code>e*I' = e*I + e'*o*L = e*I</code> so a valid signature with <code>I'</code> can created in the same way as with <code>I</code> (except that the message <code>m</code> now has to commit to <code>I'</code>).
Since <code>o</code> is at most <code>8</code> it is easy to to retry signing until there is a suitable hash <code>e</code>.</p>

<p>Interestingly, in ByteCoin this was exploited in <a href="https://monero.stackexchange.com/a/4252">a much less effective way</a>.
The attacker used a low order public key <code>P</code> requiring a low order key image <code>I</code>.
Because there are only 8 low order points on the curve (multiple representations of the same point are disallowed in CryptoNote) this attack can be only performed 8 times in one blockchain.</p>

<p>The fix implemented in Monero is to verify that each key image <code>I</code> generates a group of the prime order <code>l</code> by checking that <code>l*I = 0</code>.
If <code>I</code> actually had order <code>l' != l</code> then for <code>l*I = 0</code> to hold, <code>l</code> must be divisible by <code>l'</code> which is a contradiction because <code>l</code> is prime.
So the additional cost introduced by the fix is one scalar multiplication per transaction input.</p>

<h2>Conclusion</h2>

<p>This article gave yet another example for how cryptographic parts can not be easily repurposed.
In particular, when implementing more complex protocols based on curves with a cofactor (like for example Ed25519 or Curve25519) the group order of user supplied generator points should always be verified.
Deciding case-by-case whether that&rsquo;s necessary is quite dangerous in practice.
There is, however, potential to get rid of this bug class for some cryptosystems by eliminating cofactors through point compression (see <a href="https://eprint.iacr.org/2015/673.pdf">Decaf</a>).
Alternatively, when designing a new cryptosystem it should be considered to use a prime order curve such as <a href="https://github.com/bitcoin-core/secp256k1">secp256k1</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A problem with Monero's RingCT]]></title>
    <link href="https://jonasnick.github.io/blog/2016/12/17/a-problem-with-ringct/"/>
    <updated>2016-12-17T17:33:00+00:00</updated>
    <id>https://jonasnick.github.io/blog/2016/12/17/a-problem-with-ringct</id>
    <content type="html"><![CDATA[<p>The crypto-currency <a href="https://getmonero.org/home">Monero</a> is about to introduce a new milestone in Blockchain technology: <a href="https://eprint.iacr.org/2015/1098">RingCT</a>.
This is a scheme that allows using <a href="https://people.xiph.org/~greg/confidential_values.txt">Confidential Transactions (CT)</a> while keeping the non-interactive coin mixing typical for Monero.
CT enables hiding the transaction amounts from anyone but sender and receiver while full nodes are still able to verify that input amounts are equal to output amounts.
RingCT is currently not active in Monero; it is designed to be introduced as a hard fork early January.</p>

<p>I am a complete outsider to Monero and especially the Monero development community, but having reviewed the CT design and implementation (<a href="https://github.com/ElementsProject/secp256k1-zkp">in libsecp256k1</a>) extensively during my day job, I was very interested in the design decisions underlying RingCT.
Very quickly I found a <a href="https://twitter.com/n1ckler/status/801485209220718592">red flag</a> in the ring signature scheme called ASNL used in the range proofs.
This scheme is a new contribution by the paper and indeed turned out to be exploitable such that an <strong>attacker would be able to create coins from nothing</strong>.
You can find the exploit code on <a href="https://github.com/jonasnick/monero/commit/ad405e514c7c82bb81d7d49282fa11729420ea85">GitHub</a> and a detailed explanation in this post.</p>

<p>While writing the exploit code and preparing this blog post I learned that <a href="https://github.com/monero-project/research-lab/issues/4">an anonymous person called RandomRun reported</a> a flaw in the security proof of ASNL, which convinced the Monero devs to publish a <a href="https://github.com/monero-project/monero/releases/tag/v0.10.1">bugfix release</a> that switches to <a href="https://github.com/ElementsProject/borromean-signatures-writeup">Borromean signatures</a> (good call!).
As a result <strong>the upcoming hard fork will not be vulnerable to this exploit</strong>.
Interestingly, the error in the security proof is exactly the flip-side of the vulnerability discussed in this post.</p>

<p><strong>EDIT:</strong> The Monero community reacted to this article (<a href="https://www.reddit.com/r/Monero/comments/5j4z1e/a_problem_with_ring_ct/">see reddit</a>) but they didn&rsquo;t like its style. Also, they got the timeline of the discovery of the bug wrong.</p>

<!-- more -->


<p>I have the highest respect for RandomRun and parts of the Monero community.
It takes an incredibly strong character to drop an 0day worth tens of millions USD.
However, that the original hard fork schedule of RingCT remains unchanged despite a complete break of the system raises more than a few questions.
Even more so when the author of RingCT <a href="https://github.com/monero-project/research-lab/issues/4#issuecomment-256261207">called for more review</a> by the end of October.</p>

<h2>Aggregate Schnorr Non-linkable Ring Signature (ASNL)</h2>

<p>Confidential transactions include a range proof to prevent negative amounts.
These range proofs use a generalization of ring signatures in which
the conjunction of multiple rings is proven, for example that the prover knows the discrete logarithm of <code>(Pk1 OR Pk2) AND (Pk1 OR Pk3) AND ...</code>
The original CT scheme introduced Borromean signatures for that purpose which are based on rings of hashes and provide space savings when public keys appear more than once.</p>

<p>Instead, the RingCT paper proposes a new scheme called Aggregate Schnorr Non-linkable Ring Signature because it has &ldquo;perhaps simpler security proofs&rdquo; (RingCT paper).</p>

<p>A ASNL signature consists tuples <code>(P1_j, P2_j, L1_j, s2_j)</code> for <code>j = 1, ..., n</code> and <code>s</code> which
is supposed to prove that the signer knows the DL of <code>(P1_1 OR P2_1) AND ... AND (P1_n OR P2_n)</code>.
Let&rsquo;s consider the <code>n = 1</code> case (no conjunction) informally.
The verifier checks that</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>L1 = s*G + H(s2*G + H(L1)*P2)*P1</span></code></pre></td></tr></table></div></figure>


<p>where <code>H</code> is a hash function.</p>

<p>So either</p>

<ul>
<li>The prover knows the DL <code>x</code> of <code>P1</code> then sets</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>a, s2 &lt;- random scalar
</span><span class='line'>L1    &lt;- a*G
</span><span class='line'>s     &lt;- a - H(s2*G + H(L1)*P2)*x</span></code></pre></td></tr></table></div></figure>


<ul>
<li>Or the prover knows the DL <code>x</code> of <code>P2</code> then sets</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>a, s &lt;- random scalar
</span><span class='line'>L1   &lt;- s*G +H(a*G)*P1
</span><span class='line'>s2   &lt;- a - x*H(L1)</span></code></pre></td></tr></table></div></figure>


<p>In the case of multiple conjunctions (<code>n &gt; 1</code>), the verifier computes <code>LHS &lt;- L1_1 + ... L1_n</code> and <code>RHS &lt;- s*G + H(s2_1*G + H(L1_1)P2_1)P1_1 + ... + H(s2_n*G + H(L1_n)P2_n)P1_n</code> and checks that <code>LHS = RHS</code>.
In short, this is vulnerable because you can just choose some <code>L1_j</code> such that it cancels out the summand on the right hand side where both DLs of P1 and P2 are unknown.
In contrast, the &ldquo;proof&rdquo; of security of ASNL assumes that any adversaries knows <code>a</code> s.t. <code>a*G = L1_j</code> for all <code>j</code>.</p>

<h2>Forgery</h2>

<p><a href="https://github.com/jonasnick/monero/commit/ad405e514c7c82bb81d7d49282fa11729420ea85">Implementation</a></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Theorem
</span><span class='line'>Given any curve points P1_1, P2_1, an adversary is able to forge a ASNL signature
</span><span class='line'>(P1_j, P2_j, L1_j, s2_j) for j=1, ..., n and s where n &gt; 1 such that ASNL verify accepts.
</span><span class='line'>
</span><span class='line'>Proof
</span><span class='line'>Without loss of generality assume n = 2.
</span><span class='line'>1. Let P1_2 = xG
</span><span class='line'>    Set
</span><span class='line'>    a, s2_1, s2_2 &lt;- random scalar
</span><span class='line'>    L1_1 &lt;- a*G
</span><span class='line'>    L1_2 &lt;- H(s2_1*G + H(L1_1)*P2_1)*P1_1
</span><span class='line'>    s &lt;- a - H(s2_2*G + H(L1_2)*P2_2)*x
</span><span class='line'>    Then during verification it holds that
</span><span class='line'>    L1_1 + L1_2 = s*G + H(s2_1*G + H(L1_1)*P2_1)*P1_1 + H(s2_2*G + H(L1_2)*P2_2)*P1_2
</span><span class='line'>    &lt;=&gt; L1_1 = (a - H(s2_2*G + H(L1_2)*P2_2)*x)*G + H(s2_2*G + H(L1_2)*P2_2)*P1_2
</span><span class='line'>    &lt;=&gt; L1_1 = a*G
</span><span class='line'>2. Let P2_2 = xG
</span><span class='line'>    Set
</span><span class='line'>    a, s, s2_1 &lt;- random scalar
</span><span class='line'>    L2_2 &lt;- a*G
</span><span class='line'>    L1_1 &lt;- s*G + H(L2_2)*P1_2
</span><span class='line'>    L1_2 &lt;- H(s2_1*G + H(L1_1)*P2_1)*P1_1
</span><span class='line'>    s2_2 &lt;- a - H(L1_2)*x
</span><span class='line'>    Then during verification it holds that
</span><span class='line'>    L1_1 + L1_2 = s*G + H(s2_1*G + H(L1_1)*P2_1)*P1_1 + H(s2_2*G + H(L1_2)*P2_2)*P1_2
</span><span class='line'>    &lt;=&gt; L1_1 = s*G + H(s2_2 + H(L1_2)*P2_2)*P1_2
</span><span class='line'>    &lt;=&gt; L1_1 = s*G + H(a*G)*P1_2</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The 2016 Backdoored Cryptocurrency Contest Winner]]></title>
    <link href="https://jonasnick.github.io/blog/2016/08/31/the-2016-backdoored-cryptocurrency-contest-winner/"/>
    <updated>2016-08-31T22:43:00+00:00</updated>
    <id>https://jonasnick.github.io/blog/2016/08/31/the-2016-backdoored-cryptocurrency-contest-winner</id>
    <content type="html"><![CDATA[<p>Thanks to a quite a bit of luck I won this years version of <a href="https://underhandedcrypto.com/2016/08/17/the-2016-backdoored-cryptocurrency-contest-winner/">the Underhanded Crypto Contest</a> focused on crypto currencies. The submission consists of <a href="https://underhandedcrypto.com/2016/08/17/the-2016-backdoored-cryptocurrency-contest-winner/">a writeup</a> and &ndash; of course &ndash; <a href="https://github.com/jonasnick/2016_underhanded_crypto_contest/tree/master/submission">backdoored code</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Data-Driven De-Anonymization in Bitcoin]]></title>
    <link href="https://jonasnick.github.io/blog/2016/06/25/data-driven-de-anonymization-in-bitcoin/"/>
    <updated>2016-06-25T23:41:00+00:00</updated>
    <id>https://jonasnick.github.io/blog/2016/06/25/data-driven-de-anonymization-in-bitcoin</id>
    <content type="html"><![CDATA[<p><a href="https://www.youtube.com/watch?v=HScK4pkDNds"><img src="https://jonasnick.github.io/images/zurich-youtube.png" width="560"></a></p>

<p><a href="https://jonasnick.github.io/slides/2016-zurich-meetup.pdf"><strong>Slides</strong></a></p>

<p><strong>Abstract</strong></p>

<p>We analyse the performance of several clustering algorithms in the digital peer-
to-peer currency Bitcoin. Clustering in Bitcoin refers to the task of finding
addresses that belongs to the same wallet as a given address.
In order to assess the effectiveness of clustering strategies we exploit a vulner-
ability in the implementation of Connection Bloom Filtering to capture ground
truth data about 37,585 Bitcoin wallets and the addresses they own. In addition
to well-known clustering techniques, we introduce two new strategies, apply them
on addresses of the collected wallets and evaluate precision and recall using the
ground truth. Due to the nature of the Connection Bloom Filtering vulnerability
the data we collect is not without errors. We present a method to correct the
performance metrics in the presence of such inaccuracies.
Our results demonstrate that even modern wallet software can not protect its
users properly. Even with the most basic clustering technique known as multi-
input heuristic, an adversary can guess on average 68.59% addresses of a victim.
We show that this metric can be further improved by combining several more
sophisticated heuristics.</p>

<p><a href="https://jonasnick.github.io/papers/thesis.pdf"><center><strong>Read full thesis</strong></center></a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A Validation-cost metric for Bitcoin]]></title>
    <link href="https://jonasnick.github.io/blog/2015/12/13/validation-cost-metric/"/>
    <updated>2015-12-13T20:36:00+00:00</updated>
    <id>https://jonasnick.github.io/blog/2015/12/13/validation-cost-metric</id>
    <content type="html"><![CDATA[<p>This is the transcript of a talk I gave at the
<a href="https://scalingbitcoin.org/hongkong2015/">Scaling Bitcoin Conference</a> 2015 in Hong Kong.
See <a href="https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-December/011865.html">this mailing list post</a> by Greg Maxwell for a general summary of the scaling measures that Bitcoin Core developers are adopting.
The slides accompanying the transcript can be found <a href="https://github.com/jonasnick/jonasnick.github.com/raw/master/files/Validation-cost%20Metric.pdf">here</a>.</p>

<!-- more -->


<h2>Motivation</h2>

<p>As we&rsquo;ve seen over the last two days scalability is a multidimensional problem.
One of the main topics of research is increasing the blocksize to increase transaction throughput.
The assumption is that as technological progress is continuing and transaction throughput is increased accordingly, the cost for runnning a fully validating node stays constant.</p>

<p>However, blocksize proposals usually only directly change one aspect of full node costs &ndash; the blocksize.
The actual costs for a node are composed of multiple factors, such as the resources required to validate a block or to store the utxos.
And these factors are not necessarily in a linear relationship with each other.
This has been discussed more detailed in Mark&rsquo;s talk at the previous Scaling Bitcoin conference.</p>

<p>The most prominent example for showing non-linear relationships consists of putting as many OP_CHECKSIG operations into a single transaction as possible.
For each checksig operation, the whole transaction is hashed and a signature is verified.
Assuming 1MB blocks, it is possible to create a block that takes more than 10 minutes to validate on my 2014 laptop.
It is clear that each proposal that increases blocksize also needs a strategy to deal with these non-linearities.</p>

<p>One of those strategies is to put a hard limit the number of signature verifications and the number of bytes that are hashed for a block to be valid.
We see some problems with this approach:
First, as it stands there is no intuitive way to choose these limits nor how they grow with the blocksize.
Second, there are other factors that influence validation cost, which might not relevant now, but could get significant in bigger blocks if not properly limited.
For example, it is possible to create a 1MB block that takes 5 seconds to validate on my laptop, which just consists of as many HASH opcodes as possible.
And third, placing hard limits on certain factors completely ignores the relationship between those factors.</p>

<p>These relationships exist, because thiose factors influence validation cost in some way.
This brings us to the concept of cost metrics.</p>

<p>The goal of the cost metric approach is to tie consensus rules to actual resource requirements.
The idea is that cost of a block is a <em>function</em> of certain block properties.
As an example, the block cost could be represented by a weighted sum of block size, validation cost and utxo growth.</p>

<p>When we have agreed on such a cost metric, we can get rid of the hard limits and instead introduce a new consensus rule that blocks need to cost less than a threshold to be valid.</p>

<h2>Validation cost</h2>

<p>One aspect of a full cost function are validation-cost.
We can view validation cost as the time it take to validate a block on a reference machine.
Then we can introduce a threshold saying that a block is not allowed to exceed 30 seconds validation time on a reference machine.
In other words, we want to find a function from block features like the number of bytes that are hashed for signature validation
to validation time on the reference machine.
To do that, we assume a simple model function that states that the validation duration is a linear combination of block features,
collect data about the actual validation duration on that machine and then fit the model to the data.</p>

<p>The one dimensional situation is depicted in the right, there is one data point for each block consisting of the number of bytes that were hashed and the time it took to validate.
With this data it is possible to determine the effect or coefficient of hashing on validation time which is represented as a line in the plot.
This coefficient can then be used in a consensus rule.</p>

<p>MAYBE: If we assume that the resources involved grow at the same speed, this kind of metric can be naturally scaled by multiplying the whole equation with the inverse of the growth factor.</p>

<h2>Experiments</h2>

<p>Validation cost is affected first and foremost by OP_CHECKSIG, that is signature verification and hashing the transaction.
Bitcoin Core already limits the number of OP_CHECKSIGs but this is insufficient for our case because what counts are the number of OP_CHECKSIGs that are executed.
We built on Gavin Andresen&rsquo;s code to count those factors while validating transactions.
We also record hashing via the OP_HASH opcodes, and how many bytes are written and removed from the stack.
And the number of inputs which loosely corresponds to the number of lookups in the utxo set.
And of course we also measured our dependent variable, the ConnectBlock duration on the reference machine.</p>

<p>As a reference machine we used my laptop, which has two 3gHz i7 cores.
To collect block feature data and the corresponding ConnectBlock duration, we reindexed mainchain, testchain and custom regtest chains which for example consisted hard-to-validate blocks.
I found out that I could comfortably use the computer while using only 5GB of 8GB RAM, so I set the dbcache option to 3GB.
dbcache determines how much data is cached in memory
We ran Bitcoin Core version 0.11.2 with libsecp validation and disabled checkpoints.</p>

<h2>Result</h2>

<p>After estimating the coefficients using linear regression,
we get useful information like for each kilobyte of hashing validation takes 0.005 millisecond longer
for each signature verification it takes 0.1 millisecond longer.
Other features do not play a comparably significant role at the moment, even though it is possible
to create a block that takes around 5 seconds to validate and only consists of hash opcodes.</p>

<p>The validation cost function fit is very accurate: for a random test selection of test and mainnet we get an average absolute error of less than 4 ms.
Most importantly, the estimated function is able to predict hard-to-validate blocks very accurately: The one tested example was a block that took 130.4ms to validate, 131.7 was predicted.</p>

<h2>Cost Metric</h2>

<p>So, now we derived a validation cost metric that corresponds to
validation time on a reference machine and
we can define a new consensus rule that would require a block to
have a smaller validation cost than some threshold.
After picking a threshold, there would be a situation like in this plot, where x-axis is the block size, y-axis validation time
and the green area represents the space of valid blocks.</p>

<p>However, picking another threshold is difficult.
because there is no one size fits all solution:
(1) you don&rsquo;t want to constrain potential use cases but
(2) and you also don&rsquo;t want want to sum validation time and bandwidth worst cases.</p>

<p>On the other hand, we can try to relate bandwidth requirements and validation cost
using a simple weighted sum for example and then pick a single threshold.</p>

<p>And this is exactly the idea behind the cost-metric, find all factors affecting node cost and
how exactly they influence node costs and then pick a reasonable cost threshold.
And what this idea really entails is moving away from blocksize proposals to arguing about total node costs.</p>

<p>Now the question is how exactly do you convert bandwidth requirements, validation time to cost?
Does it make sense to trade off one second of network latency with one second of validation duration?
How do we bring additional cost factors in, like utxo set size?
How future-proof is that solution?</p>

<p>There is certainly no single correct answer to these questions.
We can, however, show the advantages of a cost function while building on existing block size proposals.
Most block size proposals consider average use at a specific maximum block size.
So in terms of cost threshold it would make a lot of sense to allow maximum sized blocks only in combination with average validation time.
In this way we can prevent blocks that have both a worst-case size and worst-case validation time.
We get the average validation duration for a specific block size using the data we collected earlier with the reference machine.</p>

<p>Also we set a hard limit validation cost of 10 seconds, which seems reasonable because the maximum validation
time on the reference machine was 6 seconds.
to the average validation time at the maximum blocksize.
Then we allow to linearly interpolate between the maximum validation time at half of the maximum blocksize</p>

<p>This shows an advantages of a cost metric: we constrain the worst case by bringing it closer to the average case,
and still allow possible future use-cases which require a lot of validation resources.</p>

<p>So far, the cost of maintaining the utxos has not played a role in Bitcoin.
In fact with a 1MB block, the worst case utxo set size increase is almost 1MB, whereas the average over the past year is an increase of around 11kilobyte.
Finding a reasonable place in the cost function is even more complicated than validation and bandwidth resources, in part because they are long-term costs.
The current situation with Bitcoin is that there is no incentive to avoid increasing the utxo set size if possible.
This can be as simple as moving bytes from the scriptSig to the scriptPubKey.
What we can do with the cost function is placing a slight incentive to include transactions that reduce the utxo set size and thereby cheapen them.
The proposed way to do this is allowing a larger validation costs when the block reduces the utxo set size.
This aligns well with the fact that blocks that sweep a lot of utxos have rather extreme validation costs due to the high ratio of inputs to outputs and we want these blocks to be valid because they are extremely beneficial.</p>

<p>In order to determine a specific function one can compute the maximum possible decrease of utxo set size for a block of maximum size.
Then linearly interpolate such that for each byte the utxo set is reduced the maximum allowed validation costs are increased until we reach let&rsquo;s say half of the remaining validation cost.
This rule does not give the utxo set size the prominent place in the cost function it would deserve but at least moves incentives in the right direction.</p>

<p>This cost function can trivially grow with the blocksize, by multiplying the validation cost limit and average validation cost with the same scaling factor.
So if the blocksize is doubled, then double max validation cost point and double max validation cost and double average transaction</p>

<p>This situation is shown in the plot for 1MB, 2MB and 4MB maximum block sizes.</p>

<p>It ensures that the worst case validation time scales as fast as the block size, which is an implicit assumption underlying many blocksize proposals.
Also it guarantees that average blocks are always allowed to have the maximum block size.</p>

<h2>Conclusion</h2>

<p>In conclusion, Bitcoin places <em>various</em> resource requirements on full nodes.
And it is essential that blocksize proposals account at least for the most important ones, or extreme worst cases are .
A cost metric helps with that because it sets the requirements in relation to each other.</p>

<p>We&rsquo;ve seen that estimating a function for validation cost only, is straightforward, when assuming a reference machine, collecting data and fitting a linear function.</p>

<p>A more complete cost function that includes bandwidth, validation and utxo requirements is difficult to derive from the bottom up.
But as we showed we can build on existing blocksize proposals to get some of the advantages of a cost metric,
* such as confining the worst-case while
* allowing to trade-off various block aspects
* and setting the right incentives.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Exploiting CSGOJackpot's weak RNG]]></title>
    <link href="https://jonasnick.github.io/blog/2015/07/08/exploiting-csgojackpots-weak-rng/"/>
    <updated>2015-07-08T00:42:00+00:00</updated>
    <id>https://jonasnick.github.io/blog/2015/07/08/exploiting-csgojackpots-weak-rng</id>
    <content type="html"><![CDATA[<p><img src="https://jonasnick.github.io/images/gaben/BwBArIM.png" width="500"></p>

<p><a href="https://csgojackpot.com">CSGOJackpot</a> is a gambling website where players bet and win Counter Strike Go &lsquo;skins&rsquo; (weapon textures).
Because these items can only be found by playing a lot of CSGo, they are quite rare and valuable,<br/>
and can be exchanged for example in Steam&rsquo;s own Marketplace.
What is fascinating about CSGOJackpot and initially captured my attention is the sheer amount
of value that is gambled away. On average, more than <strong>20,000$</strong> are thrown into the pots per hour.</p>

<p><strong>TL;DR</strong>: CSGOJackpot is a node.js app that uses <em>Math.random()</em> to determine the winning ticket. Of course, it&rsquo;s not cryptographically
secure and trivial to predict the next number given two outputs of the random number generator.
I did not try to profit from this vulnerability but for the lulz I set up a twitch stream and revealed the next winning percentage
in exchange for a drawing of Gabe Newell. See the <a href="https://github.com/jonasnick/jonasnick.github.com/blob/source/source/images/gaben/README.md">submission gallery</a> and a <a href="https://www.youtube.com/watch?v=DZrDQKbQ7r0">recording of the stream</a>.</p>

<p><strong>EDIT:</strong> There was quite some discussion about this issue on <a href="https://www.reddit.com/r/GlobalOffensive/comments/3cor52/weak_crypto_on_csgojackpot_xpost_from_rnetsec/">/r/GlobalOffensive</a>.</p>

<p><strong>EDIT 2:</strong> This vulnerability does <strong>not exist anymore</strong> in CSGOJackpot and I don&rsquo;t know a similar site which is vulnerable.</p>

<!-- more -->


<h2>Game Mechanics</h2>

<p>CSGOJackpot works like this:</p>

<ol>
<li>Start of a new round, the pot is empty.</li>
<li>A player puts up to 10 skins into the pot and receives a number of <em>tickets</em> relative to the total value of skins he deposited. For each cent he receives one ticket, the tickets are numbered and start at 0. The value of a skin is given by <a href="http://csgo.steamanalyst.com">SteamAnalyst</a>.</li>
<li>If there are less than 50 skins in the pot go to 2.</li>
<li>The site generated a random number (<em>winning percentage</em>) between 0 and 1, which is multiplied with the total number of
tickets to determine the winning ticket.</li>
<li>The player with the winning ticket wins the whole pot, except up to 5% which is kept by CSGOJackpot.</li>
</ol>


<p>In addition to guessing the winning percentage, an attacker has to know the total number of tickets to be sure to win the pot.
So, he has to try to place the last bet which can be tricky and is very difficult during times of high traffic because of huge lags.</p>

<h2>&ldquo;Breaking&rdquo; the RNG</h2>

<p>The HTML showed some signs of node.js, so my hypothesis was that the site simply uses javascript&rsquo;s Math.random() to determine the winning percentage.
Fortunately, the full winning percentage with up to 16 digits is published after the end of a round, which is exactly the amount of digits I
got when I executed Math.random() on my machine.
Node.js uses the V8 javascript engine and its <a href="https://github.com/joyent/node/blob/61c6abf00898fe00eb7fcf2c23ba0b01cf12034c/deps/v8/src/math.js#L146">implementation</a> of Math.random() (nodejs 0.12.X) is as follows:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class='javascript'><span class='line'><span class="kd">function</span> <span class="nx">MathRandom</span><span class="p">()</span> <span class="p">{</span>
</span><span class='line'>  <span class="kd">var</span> <span class="nx">r0</span> <span class="o">=</span> <span class="p">(</span><span class="nx">MathImul</span><span class="p">(</span><span class="mi">18273</span><span class="p">,</span> <span class="nx">rngstate</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&amp;</span> <span class="mh">0xFFFF</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="nx">rngstate</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;&gt;&gt;</span> <span class="mi">16</span><span class="p">))</span> <span class="o">|</span> <span class="mi">0</span><span class="p">;</span>
</span><span class='line'>  <span class="nx">rngstate</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="nx">r0</span><span class="p">;</span>
</span><span class='line'>  <span class="kd">var</span> <span class="nx">r1</span> <span class="o">=</span> <span class="p">(</span><span class="nx">MathImul</span><span class="p">(</span><span class="mi">36969</span><span class="p">,</span> <span class="nx">rngstate</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&amp;</span> <span class="mh">0xFFFF</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="nx">rngstate</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;&gt;&gt;</span> <span class="mi">16</span><span class="p">))</span> <span class="o">|</span> <span class="mi">0</span><span class="p">;</span>
</span><span class='line'>  <span class="nx">rngstate</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="nx">r1</span><span class="p">;</span>
</span><span class='line'>  <span class="kd">var</span> <span class="nx">x</span> <span class="o">=</span> <span class="p">((</span><span class="nx">r0</span> <span class="o">&lt;&lt;</span> <span class="mi">16</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="nx">r1</span> <span class="o">&amp;</span> <span class="mh">0xFFFF</span><span class="p">))</span> <span class="o">|</span> <span class="mi">0</span><span class="p">;</span>
</span><span class='line'>  <span class="c1">// Division by 0x100000000 through multiplication by reciprocal.</span>
</span><span class='line'>  <span class="k">return</span> <span class="p">(</span><span class="nx">x</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="o">?</span> <span class="p">(</span><span class="nx">x</span> <span class="o">+</span> <span class="mh">0x100000000</span><span class="p">)</span> <span class="o">:</span> <span class="nx">x</span><span class="p">)</span> <span class="o">*</span> <span class="mf">2.3283064365386962890625</span><span class="nx">e</span><span class="o">-</span><span class="mi">10</span><span class="p">;</span>
</span><span class='line'><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>This is known as <a href="https://groups.google.com/forum/#!msg/sci.stat.math/5yb0jwf1stw/ApaXM3IRy-0J">Marsaglia&rsquo;s Multiply-with-Carry</a>.
Note that the implementation used in nodejs 0.10.X uses a very similar algorithm, but it&rsquo;s implemented in C and the conversion to floating point is done differently.</p>

<p>So the RNG state has 64 bits and 32 bits immediately leak from a single output. Given two subsequent outputs one can bruteforce the remaining
32 bits of the states which takes about 30 seconds on a 3.0Ghz i7 core (implemented in C).
However, this failed to produce the correct state, so my guess was that there are some calls to Math.random() in between two winning percentages.
It turned out that the the number of calls between varies between 8 and 35 and brute forcing this required a third winning percentage and around
5 hours in expectancy. So now I had the correct state, which I verified by creating the next 50 numbers and checking if they contained the next winning percentages.
But I didn&rsquo;t find any pattern by which I could determine which of the next random numbers is going to be winning percentage.
Fortunately, there is another feature of CSGOJackpot which made this trivial.</p>

<h2>&ldquo;Provably Fair&rdquo;</h2>

<p>The site claims to be <em>provably fair</em>. But this is not really the case. What they are doing is a simple commitment to the winning percentage
, by publishing a hash <code>md5(blinding + winning percentage)</code> before the round (where the blinding is a uniformly random hexstring)
and revealing the blinding and winning percentage at the end of the round. Thus, they can not adjust the winning percentage to their
liking during or after the round. But, naturally, provable fairness implies that even the server does not know the winning percentage ahead of time.</p>

<p>However, this feature made it possible to reliably predict the next winning percentage.
I observed that the blinding just consists of two calls to Math.random() which were converted to hex with <code>toString(16).substr(2,4)</code>
and then concatenated. So now I just had to step through the next winning percentage candidates and the next blinding candidates and
check if their hash matched the commitment.</p>

<p>One more word to provably fairness. It&rsquo;s quite annoying to see CSGOJackpot and the many other sites that work similarly to
make exactly the same false claim.
I&rsquo;m not a cryptographer so take the following with a grain of salt and I&rsquo;d be happy learn if I&rsquo;m missing something important.
A truly fair scheme seems to be possible although much more complex to implement.
The underlying problem is known as <a href="https://en.wikipedia.org/wiki/Commitment_scheme#Coin_flipping">coin flipping</a>.
In a two player setting you can have each player commit to a value
and then XOR the value in the reveal phase to get a statistically independent result.
This is how for example <a href="https://satoshidice.com/provably-fair/">Satoshi Dice</a> achieves some level of fairness.</p>

<p>However, in a multi-party setting (assuming the existence of a broadcast channel), this can be trivially <a href="https://en.wikipedia.org/wiki/Sybil_attack">Sybil attacked</a>. An attacker could create multiple identities and refuse to reveal one of his commitments, if another one of his identities wins the pot.
A trivial Sybil-resistant construction would have each player loose more when not revealing than what is in the pot, but this does not seem really practical.
Another approach is to use <a href="http://www.hashcash.org/papers/time-lock.pdf">time-lock encryption</a> instead of commitments, which means that after a some time everybody can decrypt the value without having access to the key.</p>

<h2>Exploitation</h2>

<p>I didn&rsquo;t play this game at all (it would have been unfair :) ), but for the lulz I had to at least troll them a bit.
So I set up a twitch stream where I was revealing the next winning percentages in exchange for a drawing of Gabe Newell.
I privately disclosed the bug to the administrator the moment I started the stream.</p>

<p>Sorry for the bad quality in the beginning of the recording, it gets better at the 5:04 minute mark.</p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/DZrDQKbQ7r0" frameborder="0" allowfullscreen></iframe>


<p>See also the <a href="https://github.com/jonasnick/jonasnick.github.com/blob/source/source/images/gaben/README.md">submission gallery</a>.
After 2 hours of fun they fixed the issue.
Interestingly, googling &ldquo;nodejs cryptographically secure random number generator&rdquo; did not really result in plug-and-play solutions for me.
Without knowing about the pitfalls of javascript I suggested to use <code>crypto.randomBytes(4).readUIntLE(0, 4) / 0xFFFFFFFF</code> (if this is somehow wrong please
write me a message).
Unfortunately, so far they didn&rsquo;t remove the &ldquo;provable fairness&rdquo; claim.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Can miners exploit a block size increase?]]></title>
    <link href="https://jonasnick.github.io/blog/2015/06/23/can-miners-exploit-a-block-size-increase/"/>
    <updated>2015-06-23T10:49:00+00:00</updated>
    <id>https://jonasnick.github.io/blog/2015/06/23/can-miners-exploit-a-block-size-increase</id>
    <content type="html"><![CDATA[<p>We don&rsquo;t know yet. But modelling is in my opinion a useful tool to investigate potential effects.
Therefore, I used Gavin Andresen&rsquo;s mining simulator to create some more or less plausible scenarios.
You can find the <a href="https://github.com/jonasnick/bitcoin_miningsim/blob/master/analysis/README.md">resulting plots on github</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Fuzzing Bitcoin Consensus]]></title>
    <link href="https://jonasnick.github.io/blog/2015/05/09/fuzzing-bitcoin-consensus/"/>
    <updated>2015-05-09T23:39:00+00:00</updated>
    <id>https://jonasnick.github.io/blog/2015/05/09/fuzzing-bitcoin-consensus</id>
    <content type="html"><![CDATA[<p><strong>TLDR</strong> I ran <a href="http://lcamtuf.coredump.cx/afl/">afl-fuzz</a> against <a href="https://github.com/bitcoin/bitcoin/blob/15facb4aca75122b6ae0dcc6f6e112127e6a0e59/doc/release-notes/release-notes-0.10.0.md#consensus-library">libbitcoinconsensus</a> to discover interesting Bitcoin scripts and used them to search for Bitcoin reimplementations vulnerable to forking. This discovered <a href="https://github.com/btcsuite/btcd/commit/f284b9b3947eb33b91e31deec74936855feed61f">two bugs</a> in <a href="https://github.com/btcsuite/btcd">btcd</a> by Conformal.
See the <a href="https://github.com/jonasnick/bitcoinconsensus_testcases">bitcoinconsensus_testcases repository</a> for the discovered Bitcoin scripts.</p>

<!-- more -->


<h2>Forks</h2>

<p>One of the things that must not happen during regular Bitcoin operation are <em>forks</em>.
A fork occurs when there is a new block $B_{i+1}$ which is a valid successor to block $B_i$ for some set of Bitcoin nodes $N_v$ and invalid for the remaining nodes $N_{\neg v}$.
Therefore, miners in $N_v$ will mine new blocks on top of $B_{i+1}$ and miners in $N_{\neg v}$ will still mine on $B_i$.
As long as the majority of hashpower is in $N_{\neg v}$, the chain divergence will be resolved after some time, because $N_{\neg v}$&rsquo;s chain will eventually get longer than $N_v$&rsquo;s chain
and then the nodes in $N_v$ will switch to $N_{\neg v}$&rsquo;s chain.
This is due to the nature of the <a href="https://en.bitcoin.it/wiki/Block_chain">blockchain</a>: nodes always trust the longest valid chain (more exact: the chain with the most proof of work).</p>

<p>Consider for example the case of an update to the Bitcoin reference implementation that <a href="https://github.com/bitcoin/bips/blob/ced361de1d47c71e967430e17339be520b71bb1a/bip-0062.mediawiki#block-validity">restricts valid signature encodings</a>. $N_v$ are the nodes running the old Bitcoin version and $N_{\neg v}$ run the new version.
As soon as the hash power of $N_{\neg v}$ exceeds some threshold the new consensus rule can be safely activated.
In the context of Bitcoin updates this is called a <a href="https://en.bitcoin.it/wiki/Softfork">softfork</a>: a valid block becomes invalid in the new version.
On the other hand, a <a href="https://en.bitcoin.it/wiki/Hardfork">hardfork</a> occurs when an invalid block is valid in a new version, for example by <a href="http://gavinandresen.ninja/time-to-roll-out-bigger-blocks">raising the maximum block size limit</a>.
Then nodes that run the old version are represented by $N_{\neg v}$. Even if the majority of hashpower is in $N_v$, the nodes in $N_{\neg v}$ can never switch
to $N_v$&rsquo;s chain because some blocks are invalid for them.
Therefore, in the case of a hardfork all nodes are required to update.</p>

<h2>Fuzzing</h2>

<p>Forks in practice do not only happen deliberately because of updating mechanisms but can also be triggered by <a href="https://github.com/bitcoin/bips/blob/master/bip-0050.mediawiki">bugs</a>.
Bitcoin reimplementations such as libbitcoin, btcd, bitcore and toshi are particularly vulnerable to these bugs because they have to match exactly the behavior of the Bitcoin reference implementation.
In order to abstract part of the consensus critical code and allow other projects to use it, Bitcoin Core developers created the <a href="https://github.com/bitcoin/bitcoin/blob/15facb4aca75122b6ae0dcc6f6e112127e6a0e59/doc/release-notes/release-notes-0.10.0.md#consensus-library">bitcoinconsensus library</a>.
I am not aware of any reimplementation that already adopted libbitcoinconsensus.
Right now, it only has a single function bitcoinconsensus_script_verify, which takes an output <a href="https://en.bitcoin.it/wiki/Script">script</a> and a transaction and returns if the transaction is allowed to spend the output.</p>

<p>Among other conditions, a transaction is valid if the top stack item is different from 0 after script execution.
Bitcoin script is much more powerful than just verifying signatures and therefore I was curious to find interesting scripts, i.e. scripts that trigger unusual edge cases.
I&rsquo;ve recently heard about successes with <a href="http://lcamtuf.coredump.cx/afl/">afl-fuzz</a> whose heuristic using code coverage seemed to be particularly well suited for the task.
Also, it has the capability to minimize a set of inputs such that the code coverage stays the same.
After fuzzing libbitcoinconsensus for two weeks I supplied the inputs to btcd&rsquo;s <a href="https://github.com/btcsuite/btcd/tree/master/txscript">txscript</a>, a reimplementation in golang, and checked if the outputs differ.</p>

<h2>Btcd bugs</h2>

<p>The first bug I found was in btcd&rsquo;s implementation of the OP_IFDUP opcode. This opcode pushes the top stack element on the stack if it differs from 0.
Because of a type conversion in btcd, a stack element that exceeds 4 bytes would have never been copied, which differs from bitcoinconsensus&#8217; implementation of the opcode.
The second bug concerned the representation of the result of OP_EQUAL.
This opcode compares the two top stack elements and pushes the result on the stack.
In Bitcoin Core, if the comparison fails an empty byte array is pushed on the stack.
Btcd however pushed a byte array containing 0.
This means that the following script would be valid in bitcoinconsensus and invalid in btcd (Note that OP_0 pushes an empty byte array to the stack):</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>OP_0 OP_0 OP_TRUE OP_EQUAL OP_EQUAL</span></code></pre></td></tr></table></div></figure>


<p>Both bugs would have triggered hardforks. An attacker could simply broadcast a transaction with the affected scripts and it would be mined subsequently.
Btcd would have not been able to include the block into its chain and would become stuck on the last block.
Therefore, an attacker could create a block on top of btcd&rsquo;s chain paying a merchant running btcd without affecting his &lsquo;real&rsquo; coins on the main chain.
Note that the attacker would not race against the hashpower of Bitcoin miners.</p>

<p>Dave Collins from the btcd team fixed these issues very fast and additionally improved the test coverage in Bitcoin Core for the <a href="https://github.com/bitcoin/bitcoin/pull/6112">affected</a> and <a href="https://github.com/bitcoin/bitcoin/pull/6075">more</a> opcodes.
Additionally, he was so kind to award me with 0.5 bitcoin for the find.</p>

<h2>Conclusion</h2>

<p>You can find the result of the fuzzing, the code to produce them and test reimplementation in the <a href="https://github.com/jonasnick/bitcoinconsensus_testcases">bitcoinconsensus_testcases repository</a>.
If you are interested you can start fuzzing yourself and submit a pull request with new scripts you found.
Also, I&rsquo;ve executed the testcases only with btcd and bitcore so far.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Ethereum Bug Bounty Submissions]]></title>
    <link href="https://jonasnick.github.io/blog/2015/03/17/ethereum-bug-bounty-submissions/"/>
    <updated>2015-03-17T22:55:00+00:00</updated>
    <id>https://jonasnick.github.io/blog/2015/03/17/ethereum-bug-bounty-submissions</id>
    <content type="html"><![CDATA[<p>I found two vulnerabilities in the crypto currency <a href="https://ethereum.org">Ethereum</a>: <a href="https://github.com/jonasnick/eth-neg-value-tx">negative transactions</a> and <a href="https://github.com/jonasnick/ecdsaPredictableNonce/">predictable ECDSA nonce</a>.</p>

<p>As part of the <a href="https://bounty.ethdev.com/">bug bounty program</a> I was awarded with 20 Bitcoin.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Bitcoin Seeder DoS vulnerability]]></title>
    <link href="https://jonasnick.github.io/blog/2015/03/17/bitcoin-seeder-dos-vulnerability/"/>
    <updated>2015-03-17T22:23:00+00:00</updated>
    <id>https://jonasnick.github.io/blog/2015/03/17/bitcoin-seeder-dos-vulnerability</id>
    <content type="html"><![CDATA[<p>The DNS parser of the <a href="https://github.com/sipa/bitcoin-seeder">Bitcoin Seeder</a> was vulnerable to a denial of service attack. A specially crafted DNS request could trigger infinite recursive function calls that lead to a stack overflow. See the <a href="https://gist.github.com/jonasnick/62558e4b8ab43bc847c2">exploit</a>.
The vulnerability was fixed in commit <a href="https://github.com/sipa/bitcoin-seeder/commit/11e935b72020607e5c3ce85a88209bc34e427a06">11e935b</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Guessing bitcoin's P2P connections]]></title>
    <link href="https://jonasnick.github.io/blog/2015/03/06/guessing-bitcoins-p2p-connections/"/>
    <updated>2015-03-06T14:22:00+00:00</updated>
    <id>https://jonasnick.github.io/blog/2015/03/06/guessing-bitcoins-p2p-connections</id>
    <content type="html"><![CDATA[<p>The paper <a href="http://arxiv.org/abs/1405.7418">Deanonymisation of clients in Bitcoin P2P network (2014)</a> by Biryukov, Khovratovich and Pustogarov (BKP), who describe an attack on Bitcoin Core clients, has started some discussion lately.
The main idea of the paper is to first get a set of nodes $E_v$ to which your victim $v$ is directly connected to (&ldquo;entry nodes&rdquo;).
Second, for each transaction $t$ record the $10$ nodes $P_t$ which first propagate $t$.
The authors experimentally show that if $|E_v \cap P_t|$ is bigger than $3$ then there is a pretty high chance that $t$ actually originated from $v$.
However, both attack stages basically require a Sybil attack &ndash; the attacker has to be connected to a lot of nodes a lot of times.
&lsquo;A lot&rsquo; means that in their experiments they had 50 connection to each full node (~250) in the test network.
As a result, such an attack seems to be powerful, but certainly won&rsquo;t be undetected.</p>

<p>In this post I show that the first stage of the attack, namely learning the nodes a victim is directly connected to can
be done with a single connection to the victim.
In addition to BKP&rsquo;s attack, knowing all outbound peers of a client could significantly increase the success probability of a double spend.
Note that all experiments are based on Bitcoin Core 0.9.4, but 0.10.0 shows the same behavior.</p>

<p><strong>TLDR</strong> The attacker can reliably guess all of the outbound connections of a victim by making a selection from the known addresses of a victim based on the timestamp of the addresses.</p>

<p><strong>Update</strong> A <a href="https://github.com/bitcoin/bitcoin/pull/5860">fix has been merged</a> to bitcoind. The timestamp is not updated anymore when receiving a message from a connected peer. Instead, it is only updated when the peer disconnects. The fix is released in bitcoin core 0.10.1.</p>

<!-- more -->


<h2>Learning connections using addr propagation</h2>

<p>When a node $n$ connects to another peer $p$ in the network it advertises its address using the &ldquo;addr&rdquo; message.
The peer will select a number of its own peers at random which are &ldquo;responsible&rdquo; for $n$&rsquo;s address.
Then the address is forwarded to responsible peers to spread the knowledge about $n$ in the network.
The number of responsible peers is either $1$ or $2$ depending on whether the address is reachable by $p$.</p>

<p>BKP&rsquo;s attack works by recording the set of peers that first propagated a victim&rsquo;s address.
In order to have good chance to be in the set of responsible peers for the address, the attacker has to hold
a significant number of connections to each full node in the network.
Note that it is possible to have multiple connections from a single public address to a peer.</p>

<h2>The getaddr message</h2>

<p>It turns out that an attacker can simply infer the peers of a victim by sending getaddr messages to him.</p>

<p>In bitcoin, the address structures that are send via the addr message do not only contain the IP adress and port
but also a timestamp.
The timestamp&rsquo;s role is ensuring that terminated nodes vanish from the networks knowledge and it is regular refreshed by
the nodes which have an interaction (more about that later) with the peer at that address.
Bitcoin nodes usually record the addresses they hear about and send them in a reply to a getaddr using the addr message.</p>

<p>The following experiments show that an attacker can guess some or all of the direct peers of a victim
by sorting the known addresses of the victim based on the timestamp.</p>

<p>A minor obstacle is that a node replies to a single getaddr message only with maximal 2500 addrs selected uniformly at random.
In order to get a certain percentage $\tau$ of the known addresses of a node the attacker has to send multiple
getaddr messages and record the percentage that is new to her.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>S = {}
</span><span class='line'>while(true):
</span><span class='line'>    send_getaddr()
</span><span class='line'>    T = response()
</span><span class='line'>    tau' = 1 - ((|T - S|) / |T|) 
</span><span class='line'>    S &lt;- S $\cup$ T
</span><span class='line'>    if \tau' &gt; \tau
</span><span class='line'>        break
</span><span class='line'>return S</span></code></pre></td></tr></table></div></figure>


<p>Experiments show that if we wait 10 seconds after each getaddr request it takes around $3.5$ minutes to collect $\tau$ percent addresses ($13,500$ in this case).</p>

<h2>Experimental results</h2>

<p>I set up a victim node $v$, which is just a regular bitcoin node.
The attacker $a$ is a node that connects to $v$ via the P2P network and queries the known nodes of $v$.
Second, $a$ connects to $v$ via the RPC interface and gets the true peers.</p>

<p>The attacker code (btcP2PStruct) is <a href="https://github.com/jonasnick/btcP2PStruct">available on github</a>.
Thanks to the <a href="https://github.com/conformal/btcwire">btcwire</a> package it is very simple to write this kind of code.</p>

<p>You can find all the data to produce the graphs in the <a href="https://github.com/jonasnick/btcP2PStruct/data">project repository</a>.</p>

<p>First we consider the case where $v$ does not accept incoming connections (&ldquo;client&rdquo; in BKP&rsquo;s terms).
$v$ was running for 2 days and I recorded data for every hour but I will only discuss the last measurement
because the data is very similar.</p>

<div class="text-image-big">
<img src="https://jonasnick.github.io/images/guessing_btc_peers/client-histogram.png">
</div>


<p>Note that $v$ returned $12,868$ known addresses.
Also, a client usually has maximally 8 peers due to the default maximum number of outbound connections.
This implies that an attacker can not start start this attack on a client that is not connected to her.
Here we see that if the attacker obtains all peers of $v$ (without any false positives in this case).</p>

<p>Next, the case for the full node, which I left running for 8 days.</p>

<div class="text-image-big">
<img src="https://jonasnick.github.io/images/guessing_btc_peers/full_node-histogram.png">
</div>


<p>Again it is evident that an attacker can reliably determine all outbound connections of the victim using a threshold of 20 minutes.
However, inbound peers can only be detected very poorly.</p>

<p>The reason for finding all outbound peers is is <a href="https://github.com/bitcoin/bitcoin/blob/249bf0e0492758d71dc5d8fa77103b31b604979f/src/main.cpp#L4192">this logic in bitcoin-core</a> which refreshes the timestamp on every message of outbound nodes.</p>

<h2>Reducing false positives</h2>

<p>BKP mention a neat trick how to determine if two nodes $v_1$ and $v_2$ are connected.
First, the attacker connect to $v_1$ and $v_2$ and send addr messages containing bogus addresses to $v_1$.
Then, she counts the number of times one of these addresses is received from $v_2$.
However, the authors leave open how many messages you need send to be certain about the hypothesis.</p>

<p>As we already know, the address is forwarded only to two responsible nodes so we have to compute the
probabilities of our node being responsible.
Using the binomial distribution we can compute the likelihood of receiving a certain number of addresses back
given that we sent a certain number of addresses.</p>

<p>I&rsquo;ve done the math using <a href="https://github.com/jonasnick/btcP2PStruct/blob/master/prob/is_connected_prob.py">this code</a> and some assumptions regarding the structure (edges are uniformly iid).
Also, the attacker has to know or approximate the number of peers of a node, which can be done
with a similar method than the one described.
Connect two times to the victim, send and note the ratio of returned addr messages.
If you can not connect to the node, it will most likely have 8 peers.</p>

<p>This <em>theoretical</em> model shows that that if $v_1$ is a full node and $v_2$ is a client then we need about 2000 messages to determine if they are connected with 95% probability.
Similarly, if $v_1$ and $v_2$ are full nodes, the attacker needs to send 20000 messages.</p>

<p>However, in order to remain polite in the network this attack needs start from a candidate set of nodes.
Therefore, it could be a useful method to remove the false positives which were obtained with the &ldquo;getaddr&rdquo;-fingerprint.</p>

<h2>Conclusion</h2>

<p>It should be pointed out that even if you know a victim&rsquo;s entry nodes you can not simply connect to those few and listen for transactions.
This is because <a href="https://en.bitcoin.it/wiki/Satoshi_Client_Transaction_Exchange">&ldquo;trickling&rdquo;</a> prevents estimating the origin of a transaction without further assumptions or doing BKP&rsquo;s Sybil attack.
However, knowing all outbound peers of a client could significantly increase the success probability of a double spend.</p>

<p><strong>Update</strong> The fix removes the update every 20 minutes and updates on disconnect</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Privacy in BitcoinJ]]></title>
    <link href="https://jonasnick.github.io/blog/2015/02/12/privacy-in-bitcoinj/"/>
    <updated>2015-02-12T15:53:00+00:00</updated>
    <id>https://jonasnick.github.io/blog/2015/02/12/privacy-in-bitcoinj</id>
    <content type="html"><![CDATA[<p>As part of my <a href="https://github.com/jonasnick/FCW-Kernel">epic quest</a> to apply supervised machine learning to the blockchain in order to discover transaction patterns, I reviewed various wallet implementations in the hope of finding privacy leaks.</p>

<p><strong>tl;dr</strong> If you are using a wallet that is built upon BitcoinJ, such as Android Wallet, Multibit and Hive Wallet, you have almost zero <em>wire privacy</em>.
An attacker who manages to connect to your wallet is easily able to figure out all addresses you control.
This is not very likely to get fixed in the near future.</p>

<p><strong>Update:</strong> <a href="https://groups.google.com/forum/#!msg/bitcoinj/Ys13qkTwcNg/9qxnhwnkeoIJ">Mike Hearn&rsquo;s reply</a> addresses additional problems and improvements. There was also accompanying discussion on <a href="https://www.reddit.com/r/Bitcoin/comments/2vrx6n/privacy_in_bitcoinj_android_wallet_multibit_hive/">reddit</a>.</p>

<!-- more -->


<h2>Bloom Filters for SPV Nodes</h2>

<p>A <a href="https://en.wikipedia.org/wiki/Bloom_filter">Bloom filter</a> is a probabilistic data structure that is used to test whether an element is a member of a set.
Bitcoin SPV nodes that use <a href="https://github.com/bitcoin/bips/blob/master/bip-0037.mediawiki">BIP 37</a> (we call them <em>thin clients</em> from now on) <code>put</code> all public keys they are interested in into the Bloom filter and send the filter to their peers. Upon receiving a new transaction, peers <code>query</code> the Bloom filter and only relay the transaction to the BIP 37 node if the query returned true.
Thus, thin clients normally only receive the transactions they are really interested in, i.e. mostly transactions that include one of the wallet&rsquo;s keys.</p>

<p>The advantage of using a Bloom filter instead of just broadcasting all your pubkeys is that a Bloom filter is faster and more space-efficient
at the cost of some <em>false positives</em>.
That means the thin client will receive transactions that include pubkeys which were not put into the filter.
Usually, the parameters of a Bloom filter are computed such that a certain target false positive rate (<code>fp</code>) is achieved.
We want the fp rate to be relatively small (say 0.05%) to reduce bandwidth usage.</p>

<h2>Bloom Filters and Privacy</h2>

<p>BIP 37 states:</p>

<blockquote><p>Privacy: Because Bloom filters are probabilistic, with the false positive rate chosen by the client, nodes can trade off precision vs bandwidth usage. A node with access to lots of bandwidth may choose to have a high fp rate, meaning the remote peer cannot accurately know which transactions belong to the client and which don&#8217;t.</p></blockquote>


<p>This has created a misunderstanding between what is ideally possible with Bloom filters and how the reality looks like.
I&rsquo;ll focus on BitcoinJ because it is the most widely used implementation of BIP 37, but similar vulnerabilities might exist in other implementations as well.
Unfortunately, in the current BitcoinJ implementation Bloom filters are just as bad for your privacy as broadcasting your pubkeys directly to your peers.</p>

<h2>A Simple Attack</h2>

<p>The main idea behind this vulnerability is that BitcoinJ puts both pubkey and pubkeyhash into the Bloom filter which substantially reduces the false positive rate.</p>

<p>If you create a completely fresh wallet, BitcoinJ holds 271 pubkeys and computes the parameters
of the Bloom filter such that the fp rate for (271*2)+100 elements is equal to 0.05%.
Because bitcoinj initially puts only 271*2 elements into the filter (pubkey and corresponding pubkeyhash) the effective false positive rate is <code>fp=0.000146</code>.</p>

<p>The vulnerability is that if a pubkey is truly in the filter then querying both pubkey and pubkeyhash must return true.
Because the pubkeyhash is just another almost uniformly random string, the probability of a false positive for the attacker is <code>fp' = fp^2 = 0.0000000213</code>.
I obtained around 56 million pubkeys from the blockchain (mid-January), which theoretically results in <code>56 million * fp' = 1.19</code> expected false positives when scanning the blockchain.</p>

<h2>Experimental results</h2>

<p>I ran 20 crawlers since the beginning of December and collected 70,000 distinct filters until now.
These crawlers just listen for a filterload message and try to be really polite by disconnecting after 2 minutes and not sending anything.
The probability that a randomly selected DNS seed returns at least one of the crawlers is 4.3%.</p>

<p>In fact, most of the Bloom filters from recent BitcoinJ versions show a experimental false positive rate around 0.000146.
The experimental fp rate is computed by querying the filter with millions of elements which are certainly not pubkeys.
Android Wallet 4.16, 4.17, 4.18 for example use the most recent BitcoinJ version (12.2) and make up 52% of the data.
However, there is also MultiBit 0.5.18 whose effective fp rate is smaller than 0.00000001.</p>

<p>We are currently starting to analyze all filters using the described &ldquo;attack&rdquo; and we expect that this will take several weeks.
What we&rsquo;ve already seen is that the theoretical <code>fp'</code> really holds, i.e. if you create a fresh wallet and scan the whole blockchain you most likely get one false positive pubkey.</p>

<h2>(Slightly) More Difficult Attacks</h2>

<p>You might think that the problem is easily fixed by trading off bandwidth for more privacy and increase the fp rate to <code>fp = sqrt(0.0005) = 0.0224</code>.
On the one hand this might seriously impact the bandwidth of mobile clients, and on the other, there is another another general class of vulnerabilities concerning Bloom filters:
If an attacker manages to obtain multiple, different filters from the same Wallet,
he can compute the intersection of pubkeys that match the filters and therefore removes the false positive noise similar to the &ldquo;simple attack&rdquo;.
Different filters mean that they have different total size of a different Nonce.
Sending different filters can happen in BitcoinJ due to multiple reasons, for example</p>

<ul>
<li><em>Restart</em>. BitcoinJ stores the filter&rsquo;s nonce in volatile memory.</li>
<li><em>Creation of new keys</em>. When the wallet creates many new keys the filter gets &lsquo;full&rsquo; and thus has to be recomputed.</li>
<li><em>Measured false positive rate is too high</em>. BitcoinJ measures the false positive rate of transactions it receives. When it becomes too high the filter is recomputed.</li>
</ul>


<h2>Conclusion</h2>

<p>I do think this is a critical privacy leak as it doesn&rsquo;t require a sophisticated attack and wallets have practically been broadcasting all their pubkeys for years.
Not only the addresses you see in your wallet, but also a lot of your future addresses have been exposed.
From now on you should assume that the kind of bulk data collection I did is happening.
It is difficult to say how accurate and stealthy targeted attacks would be.</p>

<p>According to Mike Hearn, the creator of BitcoinJ, the problems have been known from the start but fixing these issues is far from trivial because &ldquo;lying consistently is hard&rdquo;.
I fully agree with this.
<strong>Someone needs to make it their project for a few months</strong>.</p>

<p>There are some simple ideas to slightly improve the current status such as <a href="https://twitter.com/petertoddbtc/status/559921997027610624">deploying nodes that broadcast fake bloom filters</a>.
<a href="http://www.syssec.ethz.ch/content/dam/ethz/special-interest/infk/inst-infsec/system-security-group-dam/research/publications/pub2014/acsac_gervais.pdf">Arthur Gervais et al., 2014</a> were the first to publish an academic paper on the topic and propose some more or less vague suggestions.
One idea I find interesting is that thin clients should be able to install multiple filters at their peers such that no pubkey is shared between the filters.
Thus, instead of recomputing the filter when the wallet creates new addresses, it would create an entirely fresh filter for the next keys.
One disadvantage is that at the moment multiple filters per peer is not supported by the bitcoin wire protocol.
Another issue with Bloom filters is that an attacker could safely assume that the probability is higher for two pubkeys to belong to the same person if they are closer in the transaction graph. As a countermeasure the wallet could deliberately put existing foreign pubkeys that are close into the filter.</p>

<p>I feel sorry for the people whose privacy has been potentially compromised unknowingly by malicious parties and we certainly won&rsquo;t give away the data set but nonetheless it is really exciting what can be found in the data.
If you have suggestions what to look out for and what would be interesting (not necessarily concerning machine learning) feel free to contact me.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[FCW Kernel]]></title>
    <link href="https://jonasnick.github.io/blog/2014/12/12/fcw-kernel/"/>
    <updated>2014-12-12T01:50:00+00:00</updated>
    <id>https://jonasnick.github.io/blog/2014/12/12/fcw-kernel</id>
    <content type="html"><![CDATA[<p>A Feature Coinciding Walk Kernel classifies nodes in a graph.</p>

<p>This project is a python port of <a href="https://github.com/rmgarnett/coinciding_walk_kernel">Coinciding Walk Kernels</a> (CWK) [1] and introduces an extension of the model called Feature-CWK (FCWK). If you want to jump right into some code see the <a href="https://nbviewer.ipython.org/github/jonasnick/FCW-Kernel/blob/master/benchmark.ipynb">benchmark</a>.</p>

<p>CW-Kernels deal with the problem of node classification (aka link-based classification) in which a set of features and labels for items are given just as in regular classification. In addition, a node classification algorithm accepts a graph of of items and item-item links. It has been shown that the additional information that is inherent in the network structure improves performance for certain algorithms and datasets.</p>

<p><a href="https://github.com/jonasnick/FCW-Kernel"><center><strong>Read More</strong></center></a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Reading Noun-Noun Compounds]]></title>
    <link href="https://jonasnick.github.io/blog/2013/10/14/reading-noun-noun-compounds/"/>
    <updated>2013-10-14T23:35:00+00:00</updated>
    <id>https://jonasnick.github.io/blog/2013/10/14/reading-noun-noun-compounds</id>
    <content type="html"><![CDATA[<h3>Early Influences of Compound Frequency and Semantic Transparency</h3>

<p>My bachelor thesis in Cognitive Science.
Unfortunately, I am currently not allowed to release the data nor the analysis scripts,
because the dataset is still under active research.</p>

<p><em>Abstract</em>:
This thesis evaluates psycholinguistic theories about the cognitive processing of
words. Consequently, the time-course of compound reading is analyzed using
generalized additive models in a dataset of eye movements. The theories to be
contrasted are sublexical (Taft and Forster, 1975), supralexical (Giraudo and
Grainger, 2001) vs. dual route processing (Schreuder and Baayen, 1995) and
form-then-meaning (e.g. Rastle and Davis, 2008) vs. form-and-meaning (e.g.
Feldman et al., 2009) processing.</p>

<p>As the goal is to find the best model given various predictors, some general
mechanisms of eye movements will be demonstrated, e.g. the position
in the line has substantial effects, single fixations last longer, are on shorter
words, more in the center of the word and influenced differently by frequency
measures.</p>

<p>Inspired by Kuperman et al. (2009) it is shown that already the early eye
fixations on words are guided by first constituent and compound frequency,
providing evidence for parallel dual route models.</p>

<p>Similar to Baayen et al. (2013), Latent Semantic Analysis (LSA) similarity
scores (Landauer and Dumais, 1997) permit investigating the time point of
semantic processing. The effect of LSA similarity not only shows up in the
earliest word fixations, but the data reveals that semantics plays a role even
before a word is fixated. In particular, the fixation position in the word is
more to the right, when the semantic transparency, i.e. the similarity between
compound and second constituent is high. This evidence of parafoveal semantic
processing challenges opposing findings obtained with the eye-contingent
boundary paradigm (Rayner et al., 1986). In the framework of naive
discriminative learning (Baayen et al., 2011), the effect of transparency on fixation
position reflects optimization of the landing position for accessing the orthographic
information that is most discriminative for the compound.</p>

<p><em>Keywords</em>:
reading, eye-movements, compounds, semantic similarity, morphological
processing, generalized additive model</p>

<p><a href="https://jonasnick.github.io/papers/readingCompounds.pdf"><center><strong>Read PDF</strong></center></a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Influence of Reputation on Gricean Maxims]]></title>
    <link href="https://jonasnick.github.io/blog/2013/10/14/influence-of-reputation-on-gricean-maxims/"/>
    <updated>2013-10-14T01:04:00+00:00</updated>
    <id>https://jonasnick.github.io/blog/2013/10/14/influence-of-reputation-on-gricean-maxims</id>
    <content type="html"><![CDATA[<p>As part of the course &ldquo;Game Theory and Pragmatics&rdquo; by <a href="http://www.sfs.uni-tuebingen.de/~jquinley/">Jason Quinley</a> at the <a href="http://www.sfs.uni-tuebingen.de/en/chairs.html">Institute of Linguistics</a>, I wanted to explore the influence of reputation on obeying the Gricean Maxims using data from the Q&amp;A website Stackoverflow.</p>

<p>Pragmatics is a subfield in linguistics, defined as &ldquo;dealing with the origins, uses, and effects of signs within the total behavior of the interpreters of signs&rdquo; (<a href="http://psycnet.apa.org/psycinfo/1946-02822-000">Morris, 1946</a>).
Pragmatics tries to explain why a simple sentence like &ldquo;It&rsquo;s raining.&rdquo; has a lot of different interpretations, for example(<a href="http://staff.science.uva.nl/~mfranke/Papers/Franke_PhD_thesis.pdf">Franke, 2009</a>):</p>

<ul>
<li>The speaker advises we should take an umbrella.</li>
<li>The speaker declares the picnic cancelled.</li>
<li>The speaker is sick of living in amsterdam.</li>
</ul>




<!-- more -->


<p>Herbert Grice introduced certain assumptions (<a href="http://books.google.ch/books?id=73zw8IG7mtcC&amp;lpg=PA270&amp;ots=b59dMg5U5W&amp;dq=grice%20logic%20and%20conversation&amp;lr&amp;pg=PA270#v=onepage&amp;q=grice%20logic%20and%20conversation&amp;f=false">Grice, 1975</a>) that people rely on when making pragmatic inferences in normal circumstances. He formulated the <em>Cooperative Principle</em>: &ldquo;Make your contribution such as its required, at the stage at which it occurs, by the accepted purpose or direction of the talk exchange in which your engaged.&rdquo;. Using this principle, he derived four <em>Maxims of Conversation</em>, presented as guidelines:</p>

<ul>
<li>Maxim of Quality: Do not say what you believe to be false. Do not say that for which you lack evidance.</li>
<li>Maxim of Quantity: Make your contribution as informative as is required for the current purpose of the exchange.</li>
<li>Maxim of Relation: Be relevant.</li>
<li>Maxim of Manner: Avoid ambiguity. Be brief and orderly.</li>
</ul>


<p>Grice showed that hearers can systematically interpret utterances and infer additional information that goes beyond the semantic meaning of the uttered sentence, based on the assumption that the speaker obeys the Maxims.</p>

<p>The data for studying the influence of reputation on the maxims stems from the question and answer website <a href="https://stackoverflow.com">Stackoverflow</a> (SO).
Users of the site pose programming related ques tions which others try to answer. They are encouraged to vote on the usefulness
of a question or an answer, thereby directly affecting others reputation score.
Because SO is collaboratively edited website, reputation directly determines the
privileges of a user, ranging from voting down and editing to voting on closing
or deleting questions and answers. Thus, reputation on SO is among other thing
a measure of how much the community trusts a user.
Stackoverflow provides <a href="http://blog.stackoverflow.com/2009/06/stack-overflow-creative-commons-data-dump/">monthly data dumps</a>.</p>

<div class="text-image">
<img src="https://jonasnick.github.io/InfluenceRepGrice/analyseDataset-reputationDensityPlain.png">
<div>Figure 1</div>
</div>


<p>In Figure 1 we see that there are a lot of users with low reputation, higher reputation
is getting more and more uncommon. The dashed line represents the mean.
This can be explained in part by the fact that users start out with a reputation score of one.
It looks like the distribution is following a power law, which is strengthened by Figure 2 showing that the distribution is
approximately log-normal, when users with reputation scores equal to one are excluded.</p>

<div class="text-image">
<img src="https://jonasnick.github.io/InfluenceRepGrice/analyseDataset-reputationDensityLog.png">
<div>Figure 2</div>
</div>


<p>In the following we will measure the effect of reputation by focussing on whether a question was closed or left open.
This classification task was posted on <a href="http://www.kaggle.com/c/predict-closed-questions-on-stack-overflow">kaggle</a>.</p>

<div class="text-image">
<img src="https://jonasnick.github.io/InfluenceRepGrice/analyseDataset-reputationBoxplot.png">
<div>Figure 3</div>
</div>


<p>When investigating the density of reputations given the question was closed or left open
we can see that closed questions are posed mainly by users with low reputation
(Figure 3). One interpretation is that a user with low reputation belongs
to one of two different user categories, whose members have an incentive to
choosing low effort. Those are users who have low reputation because they are
not trustful and new users who discount the future immensely because they have
a single specific question.
The inverse argument, that questions posed by users with higher reputation
have a lower probability of ending up closed is strengthened
using a logistic regression model with reputation being the only predicting variable.
This model was estimated using a dataset of 50% closed questions, whereas normally about 6% of questions end up closed.
The decision boundary is where the model estimates a 50%
probability of a closed question &ndash; it lies at a reputation of 491.
The result is that reputation is a significant influence and this model alone has an accuracy of 59.44% on test data.</p>

<p>When closing a question a moderator specifies a <a href="http://stackoverflow.com/faq#close">reason</a> for doing so, namely off topic, not constructive, not a real question, or too localized.
<strong>Interestingly, there seems to be a relation between the Gricean Maxims and the reasons for closing a question.</strong>
Questions labeled <em>off topic</em> (not related to programming) and <em>too localized</em>
(unlikely to help future visitors) clearly violate the maxim of relevance. <em>Not a
real question</em> are those that are ambiguous, vague, incomplete, overly broad, or
rhetorical, hence the maxims of manner and quantity are both violated.
The maxim of quality is violated by questions labeled <em>not constructive</em> because they are not supported by facts.
Rather, it would solicit debate, since there is no true answer.</p>

<div class="text-image">
<img src="https://jonasnick.github.io/InfluenceRepGrice/analyseDataset-reputationMaximsDensity.png">
<div>Figure 4</div>
</div>


<p>Figure 4 reveals that reputation influences which maxims are violated.
Most questions that are incomplete are posed by low reputation users, while
controversial questions are posed by high reputation users.
In other words, violations of the maxim of quality are more likely from users with high reputation,
whereas the opposite is true for the maxim of quantity and manner.
Not shown is that questions that are labeled <em>too localized</em> are in a similar reputation range like <em>not a real question</em>,
and <em>off topic</em> questions do not differ much from open questions.</p>

<p>In conclusion, even though we trust high reputation people, they are not precise about truth.
This is by no means a bad thing, as long as we take this characteristic into account when interpreting their intent.</p>

<p><strong><a href="https://github.com/jonasnick/Gricean-Classifier">Code of the analysis</a></strong></p>
]]></content>
  </entry>
  
</feed>
