<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: bitcoin | nickler's]]></title>
  <link href="http://jonasnick.github.io/blog/categories/bitcoin/atom.xml" rel="self"/>
  <link href="http://jonasnick.github.io/"/>
  <updated>2016-04-10T23:36:36+02:00</updated>
  <id>http://jonasnick.github.io/</id>
  <author>
    <name><![CDATA[Jonas Nick]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[A Validation-cost metric for Bitcoin]]></title>
    <link href="http://jonasnick.github.io/blog/2015/12/13/validation-cost-metric/"/>
    <updated>2015-12-13T20:36:00+01:00</updated>
    <id>http://jonasnick.github.io/blog/2015/12/13/validation-cost-metric</id>
    <content type="html"><![CDATA[<p>This is the transcript of a talk I gave at the
<a href="https://scalingbitcoin.org/hongkong2015/">Scaling Bitcoin Conference</a> 2015 in Hong Kong.
See <a href="https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-December/011865.html">this mailing list post</a> by Greg Maxwell for a general summary of the scaling measures that Bitcoin Core developers are adopting.
The slides accompanying the transcript can be found <a href="https://github.com/jonasnick/jonasnick.github.com/raw/master/files/Validation-cost%20Metric.pdf">here</a>.</p>

<!-- more -->


<h2>Motivation</h2>

<p>As we&rsquo;ve seen over the last two days scalability is a multidimensional problem.
One of the main topics of research is increasing the blocksize to increase transaction throughput.
The assumption is that as technological progress is continuing and transaction throughput is increased accordingly, the cost for runnning a fully validating node stays constant.</p>

<p>However, blocksize proposals usually only directly change one aspect of full node costs &ndash; the blocksize.
The actual costs for a node are composed of multiple factors, such as the resources required to validate a block or to store the utxos.
And these factors are not necessarily in a linear relationship with each other.
This has been discussed more detailed in Mark&rsquo;s talk at the previous Scaling Bitcoin conference.</p>

<p>The most prominent example for showing non-linear relationships consists of putting as many OP_CHECKSIG operations into a single transaction as possible.
For each checksig operation, the whole transaction is hashed and a signature is verified.
Assuming 1MB blocks, it is possible to create a block that takes more than 10 minutes to validate on my 2014 laptop.
It is clear that each proposal that increases blocksize also needs a strategy to deal with these non-linearities.</p>

<p>One of those strategies is to put a hard limit the number of signature verifications and the number of bytes that are hashed for a block to be valid.
We see some problems with this approach:
First, as it stands there is no intuitive way to choose these limits nor how they grow with the blocksize.
Second, there are other factors that influence validation cost, which might not relevant now, but could get significant in bigger blocks if not properly limited.
For example, it is possible to create a 1MB block that takes 5 seconds to validate on my laptop, which just consists of as many HASH opcodes as possible.
And third, placing hard limits on certain factors completely ignores the relationship between those factors.</p>

<p>These relationships exist, because thiose factors influence validation cost in some way.
This brings us to the concept of cost metrics.</p>

<p>The goal of the cost metric approach is to tie consensus rules to actual resource requirements.
The idea is that cost of a block is a <em>function</em> of certain block properties.
As an example, the block cost could be represented by a weighted sum of block size, validation cost and utxo growth.</p>

<p>When we have agreed on such a cost metric, we can get rid of the hard limits and instead introduce a new consensus rule that blocks need to cost less than a threshold to be valid.</p>

<h2>Validation cost</h2>

<p>One aspect of a full cost function are validation-cost.
We can view validation cost as the time it take to validate a block on a reference machine.
Then we can introduce a threshold saying that a block is not allowed to exceed 30 seconds validation time on a reference machine.
In other words, we want to find a function from block features like the number of bytes that are hashed for signature validation
to validation time on the reference machine.
To do that, we assume a simple model function that states that the validation duration is a linear combination of block features,
collect data about the actual validation duration on that machine and then fit the model to the data.</p>

<p>The one dimensional situation is depicted in the right, there is one data point for each block consisting of the number of bytes that were hashed and the time it took to validate.
With this data it is possible to determine the effect or coefficient of hashing on validation time which is represented as a line in the plot.
This coefficient can then be used in a consensus rule.</p>

<p>MAYBE: If we assume that the resources involved grow at the same speed, this kind of metric can be naturally scaled by multiplying the whole equation with the inverse of the growth factor.</p>

<h2>Experiments</h2>

<p>Validation cost is affected first and foremost by OP_CHECKSIG, that is signature verification and hashing the transaction.
Bitcoin Core already limits the number of OP_CHECKSIGs but this is insufficient for our case because what counts are the number of OP_CHECKSIGs that are executed.
We built on Gavin Andresen&rsquo;s code to count those factors while validating transactions.
We also record hashing via the OP_HASH opcodes, and how many bytes are written and removed from the stack.
And the number of inputs which loosely corresponds to the number of lookups in the utxo set.
And of course we also measured our dependent variable, the ConnectBlock duration on the reference machine.</p>

<p>As a reference machine we used my laptop, which has two 3gHz i7 cores.
To collect block feature data and the corresponding ConnectBlock duration, we reindexed mainchain, testchain and custom regtest chains which for example consisted hard-to-validate blocks.
I found out that I could comfortably use the computer while using only 5GB of 8GB RAM, so I set the dbcache option to 3GB.
dbcache determines how much data is cached in memory
We ran Bitcoin Core version 0.11.2 with libsecp validation and disabled checkpoints.</p>

<h2>Result</h2>

<p>After estimating the coefficients using linear regression,
we get useful information like for each kilobyte of hashing validation takes 0.005 millisecond longer
for each signature verification it takes 0.1 millisecond longer.
Other features do not play a comparably significant role at the moment, even though it is possible
to create a block that takes around 5 seconds to validate and only consists of hash opcodes.</p>

<p>The validation cost function fit is very accurate: for a random test selection of test and mainnet we get an average absolute error of less than 4 ms.
Most importantly, the estimated function is able to predict hard-to-validate blocks very accurately: The one tested example was a block that took 130.4ms to validate, 131.7 was predicted.</p>

<h2>Cost Metric</h2>

<p>So, now we derived a validation cost metric that corresponds to
validation time on a reference machine and
we can define a new consensus rule that would require a block to
have a smaller validation cost than some threshold.
After picking a threshold, there would be a situation like in this plot, where x-axis is the block size, y-axis validation time
and the green area represents the space of valid blocks.</p>

<p>However, picking another threshold is difficult.
because there is no one size fits all solution:
(1) you don&rsquo;t want to constrain potential use cases but
(2) and you also don&rsquo;t want want to sum validation time and bandwidth worst cases.</p>

<p>On the other hand, we can try to relate bandwidth requirements and validation cost
using a simple weighted sum for example and then pick a single threshold.</p>

<p>And this is exactly the idea behind the cost-metric, find all factors affecting node cost and
how exactly they influence node costs and then pick a reasonable cost threshold.
And what this idea really entails is moving away from blocksize proposals to arguing about total node costs.</p>

<p>Now the question is how exactly do you convert bandwidth requirements, validation time to cost?
Does it make sense to trade off one second of network latency with one second of validation duration?
How do we bring additional cost factors in, like utxo set size?
How future-proof is that solution?</p>

<p>There is certainly no single correct answer to these questions.
We can, however, show the advantages of a cost function while building on existing block size proposals.
Most block size proposals consider average use at a specific maximum block size.
So in terms of cost threshold it would make a lot of sense to allow maximum sized blocks only in combination with average validation time.
In this way we can prevent blocks that have both a worst-case size and worst-case validation time.
We get the average validation duration for a specific block size using the data we collected earlier with the reference machine.</p>

<p>Also we set a hard limit validation cost of 10 seconds, which seems reasonable because the maximum validation
time on the reference machine was 6 seconds.
to the average validation time at the maximum blocksize.
Then we allow to linearly interpolate between the maximum validation time at half of the maximum blocksize</p>

<p>This shows an advantages of a cost metric: we constrain the worst case by bringing it closer to the average case,
and still allow possible future use-cases which require a lot of validation resources.</p>

<p>So far, the cost of maintaining the utxos has not played a role in Bitcoin.
In fact with a 1MB block, the worst case utxo set size increase is almost 1MB, whereas the average over the past year is an increase of around 11kilobyte.
Finding a reasonable place in the cost function is even more complicated than validation and bandwidth resources, in part because they are long-term costs.
The current situation with Bitcoin is that there is no incentive to avoid increasing the utxo set size if possible.
This can be as simple as moving bytes from the scriptSig to the scriptPubKey.
What we can do with the cost function is placing a slight incentive to include transactions that reduce the utxo set size and thereby cheapen them.
The proposed way to do this is allowing a larger validation costs when the block reduces the utxo set size.
This aligns well with the fact that blocks that sweep a lot of utxos have rather extreme validation costs due to the high ratio of inputs to outputs and we want these blocks to be valid because they are extremely beneficial.</p>

<p>In order to determine a specific function one can compute the maximum possible decrease of utxo set size for a block of maximum size.
Then linearly interpolate such that for each byte the utxo set is reduced the maximum allowed validation costs are increased until we reach let&rsquo;s say half of the remaining validation cost.
This rule does not give the utxo set size the prominent place in the cost function it would deserve but at least moves incentives in the right direction.</p>

<p>This cost function can trivially grow with the blocksize, by multiplying the validation cost limit and average validation cost with the same scaling factor.
So if the blocksize is doubled, then double max validation cost point and double max validation cost and double average transaction</p>

<p>This situation is shown in the plot for 1MB, 2MB and 4MB maximum block sizes.</p>

<p>It ensures that the worst case validation time scales as fast as the block size, which is an implicit assumption underlying many blocksize proposals.
Also it guarantees that average blocks are always allowed to have the maximum block size.</p>

<h2>Conclusion</h2>

<p>In conclusion, Bitcoin places <em>various</em> resource requirements on full nodes.
And it is essential that blocksize proposals account at least for the most important ones, or extreme worst cases are .
A cost metric helps with that because it sets the requirements in relation to each other.</p>

<p>We&rsquo;ve seen that estimating a function for validation cost only, is straightforward, when assuming a reference machine, collecting data and fitting a linear function.</p>

<p>A more complete cost function that includes bandwidth, validation and utxo requirements is difficult to derive from the bottom up.
But as we showed we can build on existing blocksize proposals to get some of the advantages of a cost metric,
* such as confining the worst-case while
* allowing to trade-off various block aspects
* and setting the right incentives.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Can miners exploit a block size increase?]]></title>
    <link href="http://jonasnick.github.io/blog/2015/06/23/can-miners-exploit-a-block-size-increase/"/>
    <updated>2015-06-23T10:49:00+02:00</updated>
    <id>http://jonasnick.github.io/blog/2015/06/23/can-miners-exploit-a-block-size-increase</id>
    <content type="html"><![CDATA[<p>We don&rsquo;t know yet. But modelling is in my opinion a useful tool to investigate potential effects.
Therefore, I used Gavin Andresen&rsquo;s mining simulator to create some more or less plausible scenarios.
You can find the <a href="https://github.com/jonasnick/bitcoin_miningsim/blob/master/analysis/README.md">resulting plots on github</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Fuzzing Bitcoin Consensus]]></title>
    <link href="http://jonasnick.github.io/blog/2015/05/09/fuzzing-bitcoin-consensus/"/>
    <updated>2015-05-09T23:39:00+02:00</updated>
    <id>http://jonasnick.github.io/blog/2015/05/09/fuzzing-bitcoin-consensus</id>
    <content type="html"><![CDATA[<p><strong>TLDR</strong> I ran <a href="http://lcamtuf.coredump.cx/afl/">afl-fuzz</a> against <a href="https://github.com/bitcoin/bitcoin/blob/15facb4aca75122b6ae0dcc6f6e112127e6a0e59/doc/release-notes/release-notes-0.10.0.md#consensus-library">libbitcoinconsensus</a> to discover interesting Bitcoin scripts and used them to search for Bitcoin reimplementations vulnerable to forking. This discovered <a href="https://github.com/btcsuite/btcd/commit/f284b9b3947eb33b91e31deec74936855feed61f">two bugs</a> in <a href="https://github.com/btcsuite/btcd">btcd</a> by Conformal.
See the <a href="https://github.com/jonasnick/bitcoinconsensus_testcases">bitcoinconsensus_testcases repository</a> for the discovered Bitcoin scripts.</p>

<!-- more -->


<h2>Forks</h2>

<p>One of the things that must not happen during regular Bitcoin operation are <em>forks</em>.
A fork occurs when there is a new block $B_{i+1}$ which is a valid successor to block $B_i$ for some set of Bitcoin nodes $N_v$ and invalid for the remaining nodes $N_{\neg v}$.
Therefore, miners in $N_v$ will mine new blocks on top of $B_{i+1}$ and miners in $N_{\neg v}$ will still mine on $B_i$.
As long as the majority of hashpower is in $N_{\neg v}$, the chain divergence will be resolved after some time, because $N_{\neg v}$&rsquo;s chain will eventually get longer than $N_v$&rsquo;s chain
and then the nodes in $N_v$ will switch to $N_{\neg v}$&rsquo;s chain.
This is due to the nature of the <a href="https://en.bitcoin.it/wiki/Block_chain">blockchain</a>: nodes always trust the longest valid chain (more exact: the chain with the most proof of work).</p>

<p>Consider for example the case of an update to the Bitcoin reference implementation that <a href="https://github.com/bitcoin/bips/blob/ced361de1d47c71e967430e17339be520b71bb1a/bip-0062.mediawiki#block-validity">restricts valid signature encodings</a>. $N_v$ are the nodes running the old Bitcoin version and $N_{\neg v}$ run the new version.
As soon as the hash power of $N_{\neg v}$ exceeds some threshold the new consensus rule can be safely activated.
In the context of Bitcoin updates this is called a <a href="https://en.bitcoin.it/wiki/Softfork">softfork</a>: a valid block becomes invalid in the new version.
On the other hand, a <a href="https://en.bitcoin.it/wiki/Hardfork">hardfork</a> occurs when an invalid block is valid in a new version, for example by <a href="http://gavinandresen.ninja/time-to-roll-out-bigger-blocks">raising the maximum block size limit</a>.
Then nodes that run the old version are represented by $N_{\neg v}$. Even if the majority of hashpower is in $N_v$, the nodes in $N_{\neg v}$ can never switch
to $N_v$&rsquo;s chain because some blocks are invalid for them.
Therefore, in the case of a hardfork all nodes are required to update.</p>

<h2>Fuzzing</h2>

<p>Forks in practice do not only happen deliberately because of updating mechanisms but can also be triggered by <a href="https://github.com/bitcoin/bips/blob/master/bip-0050.mediawiki">bugs</a>.
Bitcoin reimplementations such as libbitcoin, btcd, bitcore and toshi are particularly vulnerable to these bugs because they have to match exactly the behavior of the Bitcoin reference implementation.
In order to abstract part of the consensus critical code and allow other projects to use it, Bitcoin Core developers created the <a href="https://github.com/bitcoin/bitcoin/blob/15facb4aca75122b6ae0dcc6f6e112127e6a0e59/doc/release-notes/release-notes-0.10.0.md#consensus-library">bitcoinconsensus library</a>.
I am not aware of any reimplementation that already adopted libbitcoinconsensus.
Right now, it only has a single function bitcoinconsensus_script_verify, which takes an output <a href="https://en.bitcoin.it/wiki/Script">script</a> and a transaction and returns if the transaction is allowed to spend the output.</p>

<p>Among other conditions, a transaction is valid if the top stack item is different from 0 after script execution.
Bitcoin script is much more powerful than just verifying signatures and therefore I was curious to find interesting scripts, i.e. scripts that trigger unusual edge cases.
I&rsquo;ve recently heard about successes with <a href="http://lcamtuf.coredump.cx/afl/">afl-fuzz</a> whose heuristic using code coverage seemed to be particularly well suited for the task.
Also, it has the capability to minimize a set of inputs such that the code coverage stays the same.
After fuzzing libbitcoinconsensus for two weeks I supplied the inputs to btcd&rsquo;s <a href="https://github.com/btcsuite/btcd/tree/master/txscript">txscript</a>, a reimplementation in golang, and checked if the outputs differ.</p>

<h2>Btcd bugs</h2>

<p>The first bug I found was in btcd&rsquo;s implementation of the OP_IFDUP opcode. This opcode pushes the top stack element on the stack if it differs from 0.
Because of a type conversion in btcd, a stack element that exceeds 4 bytes would have never been copied, which differs from bitcoinconsensus' implementation of the opcode.
The second bug concerned the representation of the result of OP_EQUAL.
This opcode compares the two top stack elements and pushes the result on the stack.
In Bitcoin Core, if the comparison fails an empty byte array is pushed on the stack.
Btcd however pushed a byte array containing 0.
This means that the following script would be valid in bitcoinconsensus and invalid in btcd (Note that OP_0 pushes an empty byte array to the stack):
<code>
OP_0 OP_0 OP_TRUE OP_EQUAL OP_EQUAL
</code></p>

<p>Both bugs would have triggered hardforks. An attacker could simply broadcast a transaction with the affected scripts and it would be mined subsequently.
Btcd would have not been able to include the block into its chain and would become stuck on the last block.
Therefore, an attacker could create a block on top of btcd&rsquo;s chain paying a merchant running btcd without affecting his &lsquo;real&rsquo; coins on the main chain.
Note that the attacker would not race against the hashpower of Bitcoin miners.</p>

<p>Dave Collins from the btcd team fixed these issues very fast and additionally improved the test coverage in Bitcoin Core for the <a href="https://github.com/bitcoin/bitcoin/pull/6112">affected</a> and <a href="https://github.com/bitcoin/bitcoin/pull/6075">more</a> opcodes.
Additionally, he was so kind to award me with 0.5 bitcoin for the find.</p>

<h2>Conclusion</h2>

<p>You can find the result of the fuzzing, the code to produce them and test reimplementation in the <a href="https://github.com/jonasnick/bitcoinconsensus_testcases">bitcoinconsensus_testcases repository</a>.
If you are interested you can start fuzzing yourself and submit a pull request with new scripts you found.
Also, I&rsquo;ve executed the testcases only with btcd and bitcore so far.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Bitcoin Seeder DoS vulnerability]]></title>
    <link href="http://jonasnick.github.io/blog/2015/03/17/bitcoin-seeder-dos-vulnerability/"/>
    <updated>2015-03-17T22:23:00+01:00</updated>
    <id>http://jonasnick.github.io/blog/2015/03/17/bitcoin-seeder-dos-vulnerability</id>
    <content type="html"><![CDATA[<p>The DNS parser of the <a href="https://github.com/sipa/bitcoin-seeder">Bitcoin Seeder</a> was vulnerable to a denial of service attack. A specially crafted DNS request could trigger infinite recursive function calls that lead to a stack overflow. See the <a href="https://gist.github.com/jonasnick/62558e4b8ab43bc847c2">exploit</a>.
The vulnerability was fixed in commit <a href="https://github.com/sipa/bitcoin-seeder/commit/11e935b72020607e5c3ce85a88209bc34e427a06">11e935b</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Guessing bitcoin's P2P connections]]></title>
    <link href="http://jonasnick.github.io/blog/2015/03/06/guessing-bitcoins-p2p-connections/"/>
    <updated>2015-03-06T14:22:00+01:00</updated>
    <id>http://jonasnick.github.io/blog/2015/03/06/guessing-bitcoins-p2p-connections</id>
    <content type="html"><![CDATA[<p>The paper <a href="http://arxiv.org/abs/1405.7418">Deanonymisation of clients in Bitcoin P2P network (2014)</a> by Biryukov, Khovratovich and Pustogarov (BKP), who describe an attack on Bitcoin Core clients, has started some discussion lately.
The main idea of the paper is to first get a set of nodes $E_v$ to which your victim $v$ is directly connected to (&ldquo;entry nodes&rdquo;).
Second, for each transaction $t$ record the $10$ nodes $P_t$ which first propagate $t$.
The authors experimentally show that if $|E_v \cap P_t|$ is bigger than $3$ then there is a pretty high chance that $t$ actually originated from $v$.
However, both attack stages basically require a Sybil attack &ndash; the attacker has to be connected to a lot of nodes a lot of times.
&lsquo;A lot&rsquo; means that in their experiments they had 50 connection to each full node (~250) in the test network.
As a result, such an attack seems to be powerful, but certainly won&rsquo;t be undetected.</p>

<p>In this post I show that the first stage of the attack, namely learning the nodes a victim is directly connected to can
be done with a single connection to the victim.
In addition to BKP&rsquo;s attack, knowing all outbound peers of a client could significantly increase the success probability of a double spend.
Note that all experiments are based on Bitcoin Core 0.9.4, but 0.10.0 shows the same behavior.</p>

<p><strong>TLDR</strong> The attacker can reliably guess all of the outbound connections of a victim by making a selection from the known addresses of a victim based on the timestamp of the addresses.</p>

<p><strong>Update</strong> A <a href="https://github.com/bitcoin/bitcoin/pull/5860">fix has been merged</a> to bitcoind. The timestamp is not updated anymore when receiving a message from a connected peer. Instead, it is only updated when the peer disconnects. The fix is released in bitcoin core 0.10.1.</p>

<!-- more -->


<h2>Learning connections using addr propagation</h2>

<p>When a node $n$ connects to another peer $p$ in the network it advertises its address using the &ldquo;addr&rdquo; message.
The peer will select a number of its own peers at random which are &ldquo;responsible&rdquo; for $n$&rsquo;s address.
Then the address is forwarded to responsible peers to spread the knowledge about $n$ in the network.
The number of responsible peers is either $1$ or $2$ depending on whether the address is reachable by $p$.</p>

<p>BKP&rsquo;s attack works by recording the set of peers that first propagated a victim&rsquo;s address.
In order to have good chance to be in the set of responsible peers for the address, the attacker has to hold
a significant number of connections to each full node in the network.
Note that it is possible to have multiple connections from a single public address to a peer.</p>

<h2>The getaddr message</h2>

<p>It turns out that an attacker can simply infer the peers of a victim by sending getaddr messages to him.</p>

<p>In bitcoin, the address structures that are send via the addr message do not only contain the IP adress and port
but also a timestamp.
The timestamp&rsquo;s role is ensuring that terminated nodes vanish from the networks knowledge and it is regular refreshed by
the nodes which have an interaction (more about that later) with the peer at that address.
Bitcoin nodes usually record the addresses they hear about and send them in a reply to a getaddr using the addr message.</p>

<p>The following experiments show that an attacker can guess some or all of the direct peers of a victim
by sorting the known addresses of the victim based on the timestamp.</p>

<p>A minor obstacle is that a node replies to a single getaddr message only with maximal 2500 addrs selected uniformly at random.
In order to get a certain percentage $\tau$ of the known addresses of a node the attacker has to send multiple
getaddr messages and record the percentage that is new to her.</p>

<p>```
S = {}
while(true):</p>

<pre><code>send_getaddr()
T = response()
tau' = 1 - ((|T - S|) / |T|) 
S &lt;- S $\cup$ T
if \tau' &gt; \tau
    break
</code></pre>

<p>return S
```</p>

<p>Experiments show that if we wait 10 seconds after each getaddr request it takes around $3.5$ minutes to collect $\tau$ percent addresses ($13,500$ in this case).</p>

<h2>Experimental results</h2>

<p>I set up a victim node $v$, which is just a regular bitcoin node.
The attacker $a$ is a node that connects to $v$ via the P2P network and queries the known nodes of $v$.
Second, $a$ connects to $v$ via the RPC interface and gets the true peers.</p>

<p>The attacker code (btcP2PStruct) is <a href="https://github.com/jonasnick/btcP2PStruct">available on github</a>.
Thanks to the <a href="https://github.com/conformal/btcwire">btcwire</a> package it is very simple to write this kind of code.</p>

<p>You can find all the data to produce the graphs in the <a href="https://github.com/jonasnick/btcP2PStruct/data">project repository</a>.</p>

<p>First we consider the case where $v$ does not accept incoming connections (&ldquo;client&rdquo; in BKP&rsquo;s terms).
$v$ was running for 2 days and I recorded data for every hour but I will only discuss the last measurement
because the data is very similar.</p>

<div class="text-image-big">
<img src="http://jonasnick.github.io/images/guessing_btc_peers/client-histogram.png">
</div>


<p>Note that $v$ returned $12,868$ known addresses.
Also, a client usually has maximally 8 peers due to the default maximum number of outbound connections.
This implies that an attacker can not start start this attack on a client that is not connected to her.
Here we see that if the attacker obtains all peers of $v$ (without any false positives in this case).</p>

<p>Next, the case for the full node, which I left running for 8 days.</p>

<div class="text-image-big">
<img src="http://jonasnick.github.io/images/guessing_btc_peers/full_node-histogram.png">
</div>


<p>Again it is evident that an attacker can reliably determine all outbound connections of the victim using a threshold of 20 minutes.
However, inbound peers can only be detected very poorly.</p>

<p>The reason for finding all outbound peers is is <a href="https://github.com/bitcoin/bitcoin/blob/249bf0e0492758d71dc5d8fa77103b31b604979f/src/main.cpp#L4192">this logic in bitcoin-core</a> which refreshes the timestamp on every message of outbound nodes.</p>

<h2>Reducing false positives</h2>

<p>BKP mention a neat trick how to determine if two nodes $v_1$ and $v_2$ are connected.
First, the attacker connect to $v_1$ and $v_2$ and send addr messages containing bogus addresses to $v_1$.
Then, she counts the number of times one of these addresses is received from $v_2$.
However, the authors leave open how many messages you need send to be certain about the hypothesis.</p>

<p>As we already know, the address is forwarded only to two responsible nodes so we have to compute the
probabilities of our node being responsible.
Using the binomial distribution we can compute the likelihood of receiving a certain number of addresses back
given that we sent a certain number of addresses.</p>

<p>I&rsquo;ve done the math using <a href="https://github.com/jonasnick/btcP2PStruct/blob/master/prob/is_connected_prob.py">this code</a> and some assumptions regarding the structure (edges are uniformly iid).
Also, the attacker has to know or approximate the number of peers of a node, which can be done
with a similar method than the one described.
Connect two times to the victim, send and note the ratio of returned addr messages.
If you can not connect to the node, it will most likely have 8 peers.</p>

<p>This <em>theoretical</em> model shows that that if $v_1$ is a full node and $v_2$ is a client then we need about 2000 messages to determine if they are connected with 95% probability.
Similarly, if $v_1$ and $v_2$ are full nodes, the attacker needs to send 20000 messages.</p>

<p>However, in order to remain polite in the network this attack needs start from a candidate set of nodes.
Therefore, it could be a useful method to remove the false positives which were obtained with the &ldquo;getaddr&rdquo;-fingerprint.</p>

<h2>Conclusion</h2>

<p>It should be pointed out that even if you know a victim&rsquo;s entry nodes you can not simply connect to those few and listen for transactions.
This is because <a href="https://en.bitcoin.it/wiki/Satoshi_Client_Transaction_Exchange">&ldquo;trickling&rdquo;</a> prevents estimating the origin of a transaction without further assumptions or doing BKP&rsquo;s Sybil attack.
However, knowing all outbound peers of a client could significantly increase the success probability of a double spend.</p>

<p><strong>Update</strong> The fix removes the update every 20 minutes and updates on disconnect</p>
]]></content>
  </entry>
  
</feed>
